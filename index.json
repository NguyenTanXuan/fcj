[
{
	"uri": "https://nguyentanxuan.github.io/fcj/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Tan Xuan\nPhone Number: 0857291939\nEmail: tanxuan31052004@gmail.com\nUniversity: FPT University Ho Chi Minh city\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 14/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Lessons Learned: AWS Cloud Mastery Series #1 — AI/ML/GenAI on AWS 1. Event Purpose Provide an overview of AI/ML in Vietnam.\nPresent AWS\u0026rsquo;s core AI/ML services, especially Amazon SageMaker.\nClarify Generative AI applications through Amazon Bedrock and techniques such as RAG, Prompt Engineering.\n2. Main Content Morning: Overview of AWS AI/ML Services Introduce AI/ML context and workshop objectives.\nPresent Amazon SageMaker and the full ML process: Data Prep → Labeling → Training → Tuning → Deployment.\nMLOps model and integration into the development process.\nLive demo of SageMaker Studio.\nAfternoon: Generative AI with Amazon Bedrock Introduction to Foundation Models and selection criteria (Claude, Llama, Titan\u0026hellip;). Advanced Prompt Engineering techniques: Chain-of-Thought (CoT), Few-shot learning. Explain RAG architecture and integration with Knowledge Base. Bedrock Agents and how to build multi-step workflows. Guardrails and content control mechanism. Demo of building GenAI chatbot on Bedrock. 3. Knowledge extracted Understand SageMaker as a full ML platform serving the entire model lifecycle. Understand the role of Bedrock, the characteristics of each Foundation Model and core GenAI techniques. Be able to apply RAG and Bedrock Agents to improve chatbot in Travel-Guided project. Understand the actual deployment process through demos. 4. Hands-on experience Demos demonstrate Bedrock\u0026rsquo;s fast prototyping speed and strong integration capabilities. Opportunities to interact directly with AWS experts and the AI/ML community in Vietnam. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn basic AWS, get familiar and practice with Console \u0026amp; CLI\nWeek 3: Learn AWS services, EC2, and practice deploying resources\nWeek 4: Review knowledge, prepare input for system architecture design\nWeek 5: Begin designing system architecture and integrating AWS services\nWeek 6: Backend CRUD APIs for products and orders, initial MSSQL database design\nWeek 7: User management module: registration, login, and role-based access\nWeek 8: Shopping cart and payment module development; frontend API integration\nWeek 9: Progress with great strides of the Sales Web Project\nWeek 10: Understand and apply AWS to the Sales Web Project\nWeek 11: Optimize APIs, implement search, filter, sort features; documentation\nWeek 12: Finalize backend, prepare demo and deployment documentation\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "OpenAI Open-Weight Models Now Available on AWS OpenAI’s open-weight models are now available on AWS, giving organizations and developers the ability to integrate advanced AI with greater flexibility, scalability, and control. These models are designed for text generation, reasoning, programming, and scientific workloads, with support for context lengths up to 128K tokens and adjustable reasoning levels.\nThis blog post outlines the availability of OpenAI’s new models, the architectural choices behind their AWS integration, and how developers can get started using Amazon Bedrock and Amazon SageMaker JumpStart.\nArchitecture Guidance The new models — gpt-oss-120b and gpt-oss-20b — are available through both Amazon Bedrock and Amazon SageMaker JumpStart:\nAmazon Bedrock: Provides serverless, API-driven access to models via InvokeModel or Converse. SageMaker JumpStart: Enables fine-tuning, scalable deployment, and production integration using either the console or the Python SDK. Solution architecture highlights include:\nDirect model access via Bedrock endpoints Managed deployment options through SageMaker Support for multi-service workflows and event-driven architectures While the term open-weight models is broad, some key characteristics include:\nHigh flexibility with custom deployment options Transparency through reasoning traces for interpretability Designed for embedding into event-driven and agentic workflows Compatibility with OpenAI SDKs and AWS-native services When evaluating deployment options, consider:\nIntrinsic factors: context size, performance, reasoning quality Extrinsic factors: scalability, integration requirements, reusability Human factors: developer productivity, operational ownership Technology Choices and Communication Scope Integration Scope AWS Services / Patterns to Consider Model access \u0026amp; testing Amazon Bedrock (chat playground, APIs) Fine-tuning \u0026amp; deployment Amazon SageMaker JumpStart Event-driven workflows Amazon EventBridge, AWS Lambda, Bedrock AgentCore Tool-augmented AI agents Strands Agents with Bedrock-integrated models Hub-and-Spoke Analogy for Model Integration Similar to a pub/sub hub used in microservices, AWS enables a hub-and-spoke approach for AI integration:\nModels are accessed via Bedrock endpoints Interactions are standardized through OpenAI-compatible APIs Developers can switch between different foundation models without rewriting application logic Trade-off: careful coordination and monitoring are required to manage multiple models and reasoning levels effectively.\nCore Model Access The foundation for using OpenAI models on AWS includes:\nAmazon Bedrock for serverless inference Amazon SageMaker JumpStart for managed deployment and fine-tuning OpenAI SDK compatibility for seamless integration Reasoning trace support for improved transparency These models are production-ready and optimized for integration with existing AWS workloads.\nDeveloper Entry Point Access models through the Amazon Bedrock console: request model access and start testing in the chat playground Authenticate via AWS IAM policies Use the OpenAI SDK by pointing OPENAI_BASE_URL to the Bedrock runtime endpoint Example Python code:\nfrom openai import OpenAI client = OpenAI() response = client.chat.completions.create( messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me the square root of 42 ^ 3\u0026#34;}], model=\u0026#34;openai.gpt-oss-120b-1:0\u0026#34;, stream=True ) for item in response: print(item) "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "AWS FOR SAP Introduction Migrating SAP landscapes to AWS gives enterprises the opportunity to modernize their infrastructure and leverage the elasticity of the cloud. Businesses can scale compute, memory, and storage within minutes and automate provisioning to reduce operational errors.\nHowever, applying auto scaling to SAP Application Servers introduces complexity. Traditionally, organizations over-provision resources to avoid system slowdowns during peak load periods—resulting in paying for maximum capacity even when it’s not needed.\nNative AWS components such as Auto Scaling groups, Target Groups, and Launch Templates do not fully address SAP’s architectural requirements. To solve this challenge, AWS Professional Services developed the SAP Auto Scaling solution, enabling horizontal scaling of SAP Application Servers and reducing infrastructure costs by up to 50%.\nBusiness Case for Auto Scaling The SAP Auto Scaling solution is designed to address real operational challenges, such as:\nEnsuring SAP environments dynamically adapt to workload demand—scaling out during peak load and scaling in when demand drops. Allowing administrators to proactively activate or deactivate servers for maintenance or system upgrades. Reducing operational overhead by automating routine infrastructure tasks. This solution provides the greatest value for customers who:\nExperience uneven workload cycles (e.g., lower usage during evenings or weekends). Are looking to reduce EC2 spend during off-peak hours. Operate production environments with three or more SAP Application Servers. Prefer flexible cost models without long-term commitments. Key Benefits Modernizes SAP environments by applying native cloud elasticity. Achieves significant EC2 cost savings—up to 50% compared to running On-Demand instances continuously. Supports predictable scaling patterns aligned with business demand. How the Solution Works SAP Auto Scaling automatically manages the active or inactive state of SAP Application Servers. It continuously monitors the usage of Dialog Work Processes on the ASCS (ABAP SAP Central Services) instance to determine when to scale.\nCore Functionality When load increases, the system starts a preconfigured EC2 instance and automatically launches its SAP services. Scale decisions (up or down) are based exclusively on Dialog Work Process utilization on the ASCS. Before a scale-down event, all active SAP GUI users receive an on-screen notification to save their work and reconnect. SAP Soft Shutdown ensures all batch jobs are completed before a server is turned off. Administrators can schedule start times for Application Servers. “No-scale windows” can be defined to block scaling during critical business hours. Critical servers can be marked as “always on.” Email notifications are sent for all scale events. Scale-Up Decision Tree The system follows a structured\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Measuring the Value of Learning \u0026amp; Development (L\u0026amp;D) Programs To an L\u0026amp;D leader, the importance of employee learning is obvious. But stakeholders across the organization may not immediately see the connection. Building trust requires proving the return on investment (ROI) of learning initiatives.\nIn this post, we summarize insights from Kristin Thomas (Director, Customer Value Realization at Pluralsight) on how to demonstrate the impact of upskilling and align stakeholders around measurable business value.\nKey Components of Proving L\u0026amp;D Value Kristin highlights two essential questions:\nHave employees gained new skills? Are they applying those skills to mission-critical work? Without visibility into these two factors, it’s impossible to link learning activities to business KPIs such as retention, customer satisfaction, or ticket resolution time.\nThree Steps to Demonstrating L\u0026amp;D ROI Step 1: Understand the Business Context Before launching an upskilling initiative, identify the business problem it aims to solve. Examples include:\nCompliance requirements Faster project timelines Higher product quality Talent retention Workforce productivity Clarifying the end goal helps define which outcomes should be measured.\nStep 2: Design Upskilling for the Future of Work Determine the capabilities your teams must have after training. This creates alignment with business objectives.\nAlign with existing KPIs Study the metrics used by other teams:\nHow is productivity measured? What sales metrics reflect improvement? How do you track voluntary/involuntary turnover? How is process speed measured? Choose 1–3 primary metrics to anchor L\u0026amp;D outcomes.\nEvaluate current skill \u0026amp; confidence levels Compare what employees can do now versus what they need to support future strategy.\nHigher confidence = higher likelihood of applying new skills.\nSupport learning behaviors Enable consistent learning by:\nGiving employees dedicated learning time Explaining relevance and impact Providing tools that support new workflows Step 3: Track Learning Metrics \u0026amp; Learner Preferences Engagement metrics (hours learned, logins) are not enough. Instead, track whether:\nLearning resources match role expectations Skills gained align with business goals Employees find learning relevant and usable Preferred learning methods (classroom, digital, hands-on) are provided Continuous feedback keeps programs aligned with organizational needs.\nStart Small You don’t need a full ROI model immediately. Begin with simple metrics:\nHow many employees upskilled? What skills did they gain? How much improvement occurred? Then expand into deeper performance and business outcomes.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 • Get acquainted with FCJ members • Read and take note of internship unit rules and regulations • Learn basic AWS lesson and set up coding environment 09/09/2025 09/09/2025 YouTube Playlist 2 • Learn about AWS service categories (Compute, Storage, Networking, Database, …) 10/09/2025 10/09/2025 Cloud Journey 3 • Create AWS Free Tier account • Learn AWS Console \u0026amp; CLI • Practice: create account, install \u0026amp; configure AWS CLI 11/09/2025 11/09/2025 Cloud Journey 4 • Learn basic EC2 concepts (Instance types, AMI, EBS, SSH, Elastic IP) 12/09/2025 12/09/2025 Cloud Journey 5 • Practice with EC2: launch instance, connect via SSH, attach EBS volume 13/09/2025 13/09/2025 Cloud Journey 6 • Review and summarize Week 1 achievements • Prepare notes/questions for Week 2 14/09/2025 14/09/2025 Personal Notes Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud AI Journey – Project Plan Online Shopping Website: Furious Five Fashion (FFF) AWS \u0026amp; AI-Powered E-commerce Website Solution 1. Background and Motivation 1.1 Executive Summary The client is a small-sized business specializing in fashion products for young customers. They aim to build an online clothing e-commerce website using AWS and AI, with the ability to scale flexibly, support long-term growth, and optimize operational costs.\nThe goal of this project is to shift from traditional manual management on physical servers to a flexible, intelligent, and cost-efficient cloud-based model. AWS enables the system to scale at any time, maintain fast access speed, and allow the business to focus on product development instead of infrastructure.\nThe system is designed to support end-to-end e-commerce operations: hosting and distributing web content, managing product and order databases, supporting payments, and monitoring system performance. Everything aims toward stability, security, and long-term scalability.\nThe Furious Five implementation team will accompany the client throughout the process—advising, designing the architecture, and configuring key AWS services such as Lambda, S3, DynamoDB, CloudFront, and Route 53. Beyond building the system, they also help optimize costs, ensure security, and train the internal team to manage the infrastructure effectively.\nThis project is not just a technical plan—it marks an important step in the company’s digital transformation journey.\n1.2 Project Success Criteria To ensure the success of the Furious Five Fashion project, the following clear and measurable criteria must be met, representing both business goals and technical effectiveness:\nSystem Performance The website must maintain response times under 2 seconds for all user actions, even during peak hours.\nAvailability The system must achieve 99.9% uptime, monitored and automatically reported through services like CloudWatch.\nScalability AWS infrastructure must scale automatically when traffic increases by at least 2× without causing service disruption.\nCost Optimization Monthly operating costs must remain under 30% of the projected budget, supported by AWS cost-monitoring tools such as Cost Explorer and Trusted Advisor.\nSecurity No data leaks or unauthorized access. All customer data must be protected by AWS security standards (IAM policies, encryption, HTTPS, etc.).\nDeployment \u0026amp; Operations Infrastructure must be fully deployed within 4 weeks, with complete documentation so the internal team can manage the environment effectively.\nTraining \u0026amp; Knowledge Transfer The internal technical team must be trained to confidently maintain, monitor, and secure the system without depending entirely on external support.\n1.3 Assumptions To ensure alignment and smooth execution of the FFF project, the following assumptions have been made:\nThe team already has access to AWS accounts with required permissions and has basic knowledge of essential AWS services such as Lambda, S3, IAM, and Route 53. Stable Internet connectivity is assumed since all infrastructure runs in the cloud. The team is also aware of basic security and compliance requirements before deployment.\nThe project depends on multiple external factors: stable service availability in the selected AWS region, smooth domain routing via Route 53, and effective collaboration between development teams to ensure the web application operates properly in the cloud environment.\nThe project is part of an internship, so the budget is limited—favoring free-tier usage and low-cost service configurations. Due to limited experience and tight timelines, the chosen architecture remains simple and practical.\nPotential risks include IAM misconfigurations, accidental overspending due to unused resources, AWS regional outages, service incompatibilities, or limited expertise in troubleshooting cloud systems.\nDespite these risks, the project is built on clear expectations: this is a pilot environment, with layered monitoring, backup, and cost-management strategies in place. Every challenge is considered an opportunity to learn and grow in cloud engineering.\n2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The following architecture is designed for FFF, deployed in AWS Region Singapore (ap-southeast-1). It emphasizes flexibility, security, automation, scalability, and simplicity—appropriate for an internship-level project while following AWS best practices.\nThe system follows a multi-layer design consisting of six key components:\nFrontend \u0026amp; Security Layer Users access the website through Route 53. Incoming traffic is protected with AWS WAF and optimized via CloudFront. Source code is managed and deployed through GitLab CI/CD using CloudFormation templates.\nAPI \u0026amp; Compute Layer API Gateway routes all requests to AWS Lambda, which handles application logic. Cognito manages authentication and access control.\nStorage Layer Two S3 buckets store static content (StaticData) and user uploads.\nData Layer DynamoDB stores product metadata and unstructured data. IAM ensures secure interactions between components.\nAI Layer Amazon Rekognition and Amazon Bedrock power image processing and generative AI features.\nObservability \u0026amp; Security Layer CloudWatch, SNS, and SES provide monitoring, alerting, and system notifications.\n2.2 Technical Implementation Plan Infrastructure will be managed and deployed using Infrastructure as Code (IaC) with AWS CloudFormation to ensure repeatability, stability, and ease of maintenance.\nKey AWS components—S3, Lambda, API Gateway, VPC, RDS , Cognito, and CloudWatch—will be defined entirely through CloudFormation templates stored in GitLab for version control and rollback capability.\nSensitive configurations such as IAM permissions or WAF rules require approval before deployment and follow the internal governance process with review and validation.\nAll critical system paths—from authentication to data processing—are covered by automated and manual test cases to ensure stability, security, and scalability.\nThis technical plan enables the FFF team to deploy and manage a professional cloud environment, learning real DevOps and AWS best practices.\n2.3 Project Plan The project follows Agile Scrum over 3 months, divided into 4 sprints.\nSprint Structure\nSprint Planning\nSetup AWS foundational services (S3, Route 53, IAM)\nConfigure security (WAF, CloudFront)\nIntegrate backend (Lambda, API Gateway, RDS)\nTesting, optimization, and demo preparation\nDaily Stand-up 30-minute updates to address blockers and track status.\nSprint Review Review deliverables, demo on real AWS environment, fix issues.\nRetrospective Improve DevOps workflows and automation pipeline.\nTeam Roles\nProduct Owner: Business alignment, backlog prioritization\nScrum Master: Coordination, Agile process enforcement\nDevOps/Technical Team: Backend, infrastructure, CI/CD\nMentor / AWS Partner: Architecture validation, AI testing, cost \u0026amp; security review\nCommunication Rhythm\nDaily Stand-ups (23:00)\nWeekly Sync\nEnd-of-Sprint Demo\nKnowledge Transfer After the final sprint, the technical team will deliver hands-on training on operations, monitoring (Budgets, CloudWatch), scaling, and recovery procedures.\n2.4 Security Considerations Access Management MFA for admin users; IAM roles with least privilege; auditing through CloudTrail.\nInfrastructure Security\ndedicated VPC, services are restricted using resource policies; all public endpoints use HTTPS.\nData Protection\nS3 and RDS encryption; TLS data transfer; manual periodic backups.\nDetection \u0026amp; Monitoring\nCloudTrail, Config, and CloudWatch for visibility; GuardDuty for threat detection.\nIncident Response\nClear incident workflows with log collection, analysis, and periodic simulations.\n3. PROJECT ACTIVITIES \u0026amp; DELIVERABLES 3.1 Activities \u0026amp; Deliverables Table Phase Timeline Activities Deliverables Effort(day) Infrastructure Setup Week 1 – 2 Requirements gathering, architecture design, AWS configuration (S3, CloudFront, API, Lambda, RDS, Cognito), GitLab CI/CD setup Completed AWS Architecture, Ready Infrastructure, Active CI/CD 10 Frontend Development Week 3–5 UI/UX design, FE pages (Home, Catalog, Product Detail, Cart, Checkout), API integration Completed FE (Dev), Frontend connected to API 15 Backend \u0026amp; Database Week 6–9 Lambda APIs, RDS setup, order/user/product logic, Cognito IAM setup Stable API, validated data flow, full Frontend–Backend integration 20 Testing \u0026amp; Validation Week 10–11 Functional, security, performance testing, integration testing Test Report, Validated System 5 Production Launch Week 12 Deploy to production, domain \u0026amp; SSL setup, training \u0026amp; handover Live FFF Website, Documentation Package 5 3.2 Out of Scope The following items were discussed during the requirements definition phase, but were determined to be out of scope for the FFF Web Clothing project at the current stage.\nItems out of scope include:\nMobile App development for the system (Android/iOS). Integration of real-world inventory, shipping and logistics management systems (Fast Delivery, GHN, Viettel Post, etc.). Advanced administrative functions such as multi-level authorization, automatic revenue reporting, advanced statistical charts. Integration of third-party CRM (Customer Relationship Management) or ERP (Enterprise Resource Planning). Use of AWS services with higher, more expensive automatic security features. Integration of real-world payment gateways (VNPay, Momo, ZaloPay, Stripe, PayPal, etc.) Multilingual and multi-currency 3.3\tPATH TO PRODUCTION Phase 1 – Prototype (POC)\nActivities: Build a test version of FFF Web Sales with basic interface (Home, Category, Product Details, Cart).\nConnect backend via API Gateway – Lambda – DynamoDB.\nDeploy static website on Amazon S3 + CloudFront.\nConfigure admin account and demo trial order process.\nPhase 2 – Complete system and test (UAT)\nActivities:\nAdd user functions: login/register, authentication via AWS Cognito.\nAdd trial payment feature via sandbox.\nAdd monitoring with Amazon CloudWatch and error handling log.\nPerform internal user testing (User Acceptance Test).\nPhase 3 - Official Operation Deployment (Production)\nActivities:\nMove the entire system from the test environment to Production AWS. Configure Route53 for the official domain and SSL certificate via AWS Certificate Manager. Set up external security with AWS WAF. Optimize S3 capacity and CDN structure on CloudFront. Phase 4 – Stabilization \u0026amp; optimization after deployment\nActivities:\nMonitor actual AWS costs, optimize storage and logs.\nAdjust Lambda configuration to reduce cold start time.\nPerform periodic backups and test data recovery.\nUpdate operational documentation for the administration team.\nSummary\nThe FFF Web Sales system has been successfully deployed on the AWS Serverless platform with a cost-optimized, highly secure and scalable architecture. The stages were completed on schedule, ensuring that all functions were tested, refined and operated stably. The project is now ready to expand real users and integrate more advanced e-commerce features.\n4. AWS COST ESTIMATION Estimated monthly cost:\nRoute 53 : $1.00 AWS WAF : $5.00 CloudFront: $3.90 Amplify: $10.00 S3 (StaticData) : $0.50 S3 (Uploads): $0.75 S3 (Bucket): $0.75 AWS Lambda: $0.25 API Gateway: $3.50 Amazon Bedrock: $3.00 RDS: $21.00 IAM: Free CloudWatch: $2.00 SNS: $0.10 SES: $0.20 CloudFormation: Free GitLab CI/CD : $3.00 WS Config / Setup \u0026amp; Test migration tools $5.00 (1 lần) Estimated monthly total cost: ~ $50.00 – $55.00 USD KEY ASSUMPTIONS\nRegion: ap-southeast-1 (Singapore). User access: 100–200/month. The system is always running 24/7 but low load. Mostly API via Lambda. Small data (\u0026lt;30GB total). CI/CD 1–2 deployments per week. Free-tier is valid for the first 12 months. AI is used for demo purposes, not large-scale inference.\nSUGGESTED COST OPTIMISATION\nEnable S3 Intelligent-Tiering to automatically move less frequently accessed data. Limit CloudWatch Logs to 14–30 days. Use AWS Budgets to alert if it exceeds $40/month. If deploying long-term → consider Savings Plan for Lambda (30–40% reduction).\n5. Team Partner Executive Sponsor Name: Nguyen Gia Hung Title: FCJ Vietnam Training Program Director Description: Responsible for overall oversight of the FCJ internship program\nEmail/contact information: hunggia@amazon.com|\nProject Stakeholders Name: Van Hoang Kha Title: Support Teams Description: Responsible for overall supervision of the FCJ internship program as the Executive Support person.\nEmail/Contact information: Khab9thd@gmail.com\nPartner Project Team (Furious Five Internship Team)\nName: Duong Minh Duc Title: Project Team Leader\nDescription: Manage progress, coordinate work between the team and mentor, Manage AWS infrastructure deployment (S3, Lambda, IAM)\nEmail/Contact information: ducdmse182938@fpt.edu.vn\nName: Quach Nguyen Chi Hung\nTitle: Member\nDescription: In charge of UI/UX and user interface\nEmail/Contact information: bacon3632@gmail.com\nName: Nguyen Tan Xuan\nTitle: Member\nDescription: Responsible for Backend and server logic processing\nEmail/Contact information: xuanntse184074@fpt.edu.vn\nName: Nguyen Hai Dang\nTitle: Member\nDescription: Manage AWS infrastructure deployment (S3, Lambda, IAM) and AI chat bot integration\nEmail/Contact information: dangnhse184292@fpt.edu.vn\nName: Pham Le Huy Hoang\nTitle: Member\nDescription: Testing, quality assurance and GitLab CI/CD integration, and AI chat bot integration\nEmail/Contact information: hoangplhse182670@fpt.edu.vn\nProject Escalation Contacts\nName: Duong Minh Duc\nTitle: Project Team Leader\nDescription: Represent the internship team to contact the mentor and sponsor directly\nEmail/Contact information: ducdmse182938@fpt.edu.vn\n6. RESOURCES \u0026amp; COST ESTIMATES Resources Role Responsibilities Fee (USD)/Hour Solution Architect(1) Design overall solution, ensure technical feasibility, select appropriate AWS service 35 Cloud Engineer(2) Deploy AWS infrastructure, configure services (S3, IAM\u0026hellip;), test and optimize system 20 Project Manager (1) Monitor progress, coordinate team, manage scope and risk for the project. 15 Support / Documentation (1) Prepare communication documents, user manuals and summary reports. 10 Estimate costs by project phase Project Phase Solution Architect (hrs) 2 Engineers (hrs) Project Manager (hrs) Project Management/Suppor(hrs) Total Hours Survey \u0026amp; Solution Design 53 40 13 13 119 Implementation \u0026amp; Testing 67 160 21 19 267 Handover \u0026amp; Support 27 53 21 19 120 Total Hours 147 253 55 51 506 Total Amount $5145 $5060 $825 $510 $11540 Cost Contribution Allocation Party Contribution (USD) % Contribution Customer 4616 40% Partner (Furious Five) 2308 20% AWS 4616 40% "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Lessons Learned: AWS Cloud Mastery Series #2 — DevOps on AWS 1. Event Objectives Enhance understanding of DevOps mindset and culture. Clarify AWS CI/CD services: CodeCommit, CodeBuild, CodeDeploy, CodePipeline. Gain knowledge of Infrastructure as Code with CloudFormation and AWS CDK. Learn about Container services (ECS, EKS, App Runner) and observability systems (CloudWatch, X-Ray). 2. Main Contents Morning: CI/CD Pipeline \u0026amp; Infrastructure as Code DevOps Mindset and DORA metrics (Deployment Frequency, MTTR…).\nCI/CD System:\nSource: CodeCommit and Git strategy.\nBuild/Test: CodeBuild.\nDeployment: CodeDeploy with Blue/Green, Canary, Rolling.\nPipeline: automation with CodePipeline.\nDemo of a complete CI/CD pipeline.\nInfrastructure as Code:\nCloudFormation: templates, stacks, drift detection.\nAWS CDK: constructs, reusable patterns, multi-language.\nDemo and discussion of appropriate IaC options.\nAfternoon: Container \u0026amp; Observability Container Services: Docker: basic containerization knowledge. Amazon ECR: storage, scanning and lifecycle image management. Amazon ECS/EKS: deployment, scaling, orchestration. App Runner: PaaS for containers. Monitoring \u0026amp; Observability: CloudWatch: metrics, logs, alarms, dashboards. AWS X-Ray: distributed tracing for microservices. Best Practices: Feature flags, A/B testing. Incident management and postmortems. Demo and case study: evaluate deployment strategy for microservices. 3. Knowledge learned Understand DevOps thinking and the meaning of DORA metrics. Understand how to coordinate between Code* services to build a complete CI/CD. Have a solid foundation in IaC (CloudFormation, CDK) to optimize multi-stack architecture. Clearly see the importance of Monitoring/Tracing through CloudWatch and X-Ray. Understand container architecture orientation with ECS/EKS for large-scale systems. Practical CI/CD \u0026amp; IaC demo, immediately applicable to projects. 4. Experience the event Clear, practical content. Demo and case study support easy visualization of the deployment process. Opportunity to connect directly with experts and the DevOps community in Vietnam. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/15/2025 09/15/2025 3 - Learn about AWS Backup: centralized backup management + How Backup Plan works + Backup schedule \u0026amp; lifecycle + Tagging \u0026amp; monitoring backups 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn AWS Compute basics: + EC2 overview + Instance families \u0026amp; types + AMI (Amazon Machine Image) + Purchasing options (On-demand, Reserved, Spot) + Nitro system \u0026amp; hypervisor 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Dive into container services: + ECS (Elastic Container Service) + EKS (Elastic Kubernetes Service) + Fargate (serverless container) + ECR (Elastic Container Registry) 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn optimization \u0026amp; management: + Compute Optimizer + Auto Scaling + Hibernation - Practice: + Launch EC2 instance \u0026amp; connect + Try backup setup + Explore scaling options 09/19/2025 09/20/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood the role of AWS Backup as a centralized management service, while actual backup capability depends on each AWS service. Learned about EC2 instances, their operating systems, architectures (x86, ARM/Graviton), purchasing models, AMIs, and Nitro Hypervisor. Explored container services: ECS (simple), EKS (Kubernetes-based), Fargate (serverless), and ECR for storing container images. Understood optimization tools and features: Compute Optimizer, Auto Scaling, and Hibernation. Practiced with EC2: launching, connecting via SSH, attaching volumes, and trying backup configuration. Acquired a clear overview of how to choose compute and backup solutions on AWS based on cost, performance, and management needs. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Lessons Learned: AWS Cloud Mastery Series #3 — AWS Well-Architected Security Pillar 1. Event Purpose Introduce the role of Security Pillar in AWS Well-Architected Framework. Present 5 pillars of security: IAM, Detection, Infrastructure Protection, Data Protection, Incident Response. Provide best practices and real-world scenarios to protect applications in the Cloud environment. 2. Main Content Pillar 1 — Identity \u0026amp; Access Management (IAM) Principles: Least Privilege, Zero Trust, Defense in Depth. Modern IAM: Avoid long-term credentials, prioritize IAM Roles and managed policies. IAM Identity Center: SSO, Permission Sets, centralized access management. Multi-account security: Service Control Policies and Permission Boundaries. Mini demo: Test IAM policy and simulate access behavior. Pillar 2 — Detection Continuous monitoring with CloudTrail (org-level), GuardDuty, Security Hub. Multi-tier logging: VPC Flow Logs, ALB logs, S3 access logs. Automated alerts via Amazon EventBridge. Pillar 3 — Infrastructure Protection Network security: network separation (public/private VPC). Defense mechanisms: Security Groups vs NACLs, AWS WAF, Shield, Network Firewall. Workload security: EC2, ECS/EKS at basic level. Pillar 4 — Data Protection At-rest and in-transit data encryption (S3, EBS, RDS, DynamoDB). KMS for key management; Secrets Manager and Parameter Store for secrets management. Data classification and access guardrails setup. Pillar 5 — Incident Response Incident Response lifecycle according to AWS. Build IR playbook and automate with Lambda/Step Functions. Sample scenario: exposed IAM keys, S3 public exposure, malware detection on EC2. 3. Learnings Understand the 5 pillars of Security Pillar and Shared Responsibility Model. Apply advanced IAM: Identity Center, SCPs, avoid using long-term credentials. Understand the importance of Data Security: KMS, secrets management. Know how to build and automate Incident Response through serverless workflows. 4. Experience the event The workshop summarizes the learning chain, strengthening the security foundation before completing the project. The IAM Identity Center and Secrets Manager sections directly solve the problem of authentication and API key management of the group. IR scenarios (like S3 public exposure) help reinforce project security policies. Q\u0026amp;A session to further guide the AWS Security Specialty certification path. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - OpenAI Open-Weight Models Now Available on AWS This blog introduces the availability of OpenAI’s open-weight models on AWS, enabling developers and organizations to build AI applications with greater flexibility, scalability, and transparency. You will learn how these models support text generation, reasoning, programming, and scientific workloads, how to access them through Amazon Bedrock and Amazon SageMaker JumpStart, and how features like 128K token context length and reasoning trace transparency can be leveraged to integrate AI into scalable, secure, and production-ready workflows.\nBlog 2 - AWS FOR SAP The AWS SAP Auto Scaling solution reduces EC2 costs by up to 50% by dynamically adjusting Application Servers based on real-time workload demand. It eliminates over-provisioning, scaling resources up or down while protecting active jobs via Soft Shutdown protocols. This automated approach optimizes efficiency and performance for enterprises with fluctuating usage patterns.\nBlog 3 - Measuring the Value of Learning \u0026amp; Development (L\u0026amp;D) Programs This blog demonstrate the ROI of Learning \u0026amp; Development (L\u0026amp;D), leaders must link upskilling directly to business outcomes rather than just tracking engagement metrics. This involves identifying specific organizational problems, aligning training with existing KPIs, and verifying that employees apply new skills to mission-critical tasks. By starting with clear metrics on skill acquisition and application, organizations can build trust with stakeholders and prove the tangible value of their learning programs.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand AWS Database services and their use cases. Learn about AWS Security concepts and services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Amazon RDS and Aurora - Learn about Multi-AZ Deployment, Read Replica, Automatic Backup, Snapshot 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice monitoring tools: CloudWatch, Enhanced Monitoring, Performance Insight - Introduction to AWS DMS \u0026amp; Schema Conversion Tool for database migration 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon DynamoDB (NoSQL) - Understand Purpose-built Database concept and suitable use cases 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Data Lake concepts on AWS - Services: S3, Glue Crawler, Glue Data Catalog, Athena 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Security basics: Shared Responsibility Model - Learn AWS IAM for identity \u0026amp; access management - Explore AWS Organizations and AWS SSO for account \u0026amp; identity management 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ 7 - Introduction to Compliance \u0026amp; Security Hub - Review overall AWS Security practices and governance 09/27/2025 09/27/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood Amazon RDS, Aurora, DynamoDB, and Purpose-built Databases. Practiced monitoring with CloudWatch, Enhanced Monitoring, and Performance Insight. Learned database migration tools: AWS DMS \u0026amp; Schema Conversion Tool. Gained knowledge about Data Lake architecture using S3, Glue, and Athena. Understood the Shared Responsibility Model for AWS Security. Used IAM for access management and explored AWS Organizations \u0026amp; SSO for multi-account governance. Introduced to Compliance \u0026amp; Security Hub for compliance and governance monitoring. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 — AI/ML/GenAI on AWS\nTime: 09:00 November 15, 2025\nLocation: 26th Floor, Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 2 Event name: AWS Cloud Mastery Series #2 — DevOps on AWS\nTime: 09:00 November 17, 2025\nLocation: 26th Floor, Bitexco Building, No. 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 3 Event name: AWS Cloud Mastery Series #3 — According to AWS Well-Architected Security Pillar\nTime: 09:00 November 29/11/2025\nLocation: 26th Floor, Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Connect and collaborate with team members to discuss the overall direction of the clothing e-commerce website project. Learn about the backend structure, technology stack, and define team roles. Start exploring how to integrate AWS services (such as S3, EC2, and RDS) into the backend system. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Meet team members and discuss project overview - Define main roles and responsibilities (Backend, Frontend, UI/UX, etc.) 09/29/2025 09/29/2025 Team meeting notes 3 - Explore backend project structure (Spring Boot) - Set up local development environment - Learn about existing API structure and database schema 09/30/2025 09/30/2025 Internal GitHub repo 4 - Research AWS services for potential integration + AWS S3 for image storage + AWS RDS for database hosting + AWS EC2 for backend deployment 10/01/2025 10/01/2025 AWS Documentation 5 - Discuss deployment strategy with the team - Plan backend endpoints related to product management (CRUD for clothing items) 10/02/2025 10/02/2025 Internal Docs 6 - Begin designing and coding API endpoints + Product listing + Category management + Image upload testing (with AWS S3) 10/03/2025 10/03/2025 Spring Boot Docs Week 4 Achievements: Connected and communicated effectively with team members to understand the project’s goals and workflow. Clearly defined the backend role and successfully set up the local development environment. Understood the overall architecture of the clothing e-commerce website, including database design, service layer, and API flow. Researched and evaluated AWS services for integration: AWS S3 — for storing product images. AWS RDS — for hosting the MySQL database. AWS EC2 — for backend deployment. Contributed to designing product-related APIs (CRUD operations). Successfully tested image upload and retrieval using AWS S3. Strengthened knowledge of backend workflows and AWS integration in real-world web applications. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Begin shaping the system architecture for the e-commerce clothing website project. Research and design the system architecture integrated with AWS services. Visualize and document the project architecture using appropriate tools. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Review the project’s functional requirements.\n- Identify system modules and backend responsibilities. 06/10/2025 06/10/2025 Internal notes 2 - Research common web architectural models (3-tier, MVC, microservices, etc.). 07/10/2025 07/10/2025 AWS Documentation 3 - Learn AWS services used in the project:\n+ EC2\n+ RDS\n+ S3\n+ CloudFront\n+ CloudWatch 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Design and draw the system architecture diagram for the e-commerce project integrating AWS services. 09/10/2025 09/10/2025 draw.io 5 - Finalize and document the AWS-integrated architecture for group discussion and refinement. 10/10/2025 10/10/2025 Team meeting notes Week 5 Achievements: Understood the importance of system architecture in developing scalable web applications. Learned how to combine core AWS services (EC2, RDS, S3, CloudFront, CloudWatch) in an integrated architecture. Designed and drew the architecture diagram for the clothing e-commerce website with AWS integration. Defined how backend services interact with AWS for data storage, deployment, and monitoring. Documented the initial architecture for future implementation and team review. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd. from 8/9/2025 to 12/9/2025, I had the opportunity to apply my academic knowledge in a real-world professional setting while gaining practical experience.\nI participated in the First Cloud Journey program, which strengthened my communication abilities, enhanced my financial management skills, and expanded my understanding of AWS services and cloud computing.\nIn terms of professionalism, I consistently worked to complete all assigned tasks to a high standard, followed workplace guidelines, and actively collaborated with colleagues to improve team efficiency.\nNo. Criteria Description Good Fair Avg 1 Professional Knowledge \u0026amp; Skills Correctly understands the field, applies to practice, proficient with tools, high work quality ✅ ☐ ☐ 2 Learning Ability Absorbs new knowledge quickly, proactive in learning ☐ ✅ ☐ 3 Proactivity (Initiative) Self-starts tasks, does not wait for assignment ✅ ☐ ☐ 4 Sense of Responsibility Completes on time, ensures quality ✅ ☐ ☐ 5 Discipline Adheres to punctuality, regulations, and work processes ☐ ✅ ☐ 6 Progressive Spirit Willing to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presents ideas clearly, reports work coherently ☐ ✅ ☐ 8 Teamwork Cooperates effectively with colleagues, participates actively in team activities ✅ ☐ ☐ 9 Professionalism Respects colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving Skills Identifies problems, offers suitable solutions, possesses creative thinking ☐ ✅ ☐ 11 Contribution to Team/Project Work effectiveness, proposes new ideas, recognized by the team ☐ ✅ ☐ 12 Overall Assessment General evaluation of the entire internship process ☐ ✅ ☐ Needs Improvement Improve self-discipline and strictly comply with the company’s regulations. Enhance problem-solving thinking. Learn to handle situations more quickly and safely. Set clearer goals "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Implement and test backend API endpoints for the clothing e-commerce website. Integrate AWS S3 for file storage and manage product images. Set up CI/CD pipeline for automated deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Implement product CRUD API endpoints - Test API using Postman or Insomnia - Validate request/response data 13/10/2025 13/10/2025 Spring Boot Documentation 2 - Set up AWS S3 bucket for product images - Configure bucket policies and CORS - Implement image upload/download functionality 14/10/2025 14/10/2025 AWS S3 Documentation 3 - Integrate file upload with product creation - Handle image resizing and optimization - Test image upload flow end-to-end 15/10/2025 15/10/2025 ImageIO, AWS SDK Documentation 4 - Set up GitHub Actions for CI/CD - Configure automated testing - Create deployment workflow 16/10/2025 16/10/2025 GitHub Actions Documentation 5 - Deploy application to AWS EC2 - Configure load balancer and auto-scaling - Set up monitoring with CloudWatch 17/10/2025 17/10/2025 AWS EC2, CloudWatch Documentation Week 6 Achievements: Successfully implemented complete CRUD operations for product management. Integrated AWS S3 for secure and scalable image storage. Configured proper bucket policies for secure file access. Built automated CI/CD pipeline using GitHub Actions. Deployed application to AWS EC2 with proper monitoring. Implemented image optimization to improve performance. Set up automated testing pipeline to ensure code quality. Configured CloudWatch for application monitoring and alerting. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. Team members at FCJ are always willing to help whenever I face difficulties, even outside of working hours. The workspace is clean and comfortable, which helps me stay focused. However, I think the company could organize more social events or team-bonding activities to help everyone connect better.\n2. Support from Mentor / Team Admin\nMy mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports all necessary procedures, documents, and ensures I have what I need to work effectively. I really appreciate that my mentor allows me to try and solve problems on my own instead of giving the answer directly.\n3. Relevance of Work to Academic Major\nThe tasks I’m assigned are aligned with what I learned at university while also introducing me to new areas I haven\u0026rsquo;t explored before. This helps me reinforce my foundational knowledge while gaining more practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. My mentor also shared valuable real-world experiences that helped me shape my career path more clearly.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously, but maintains a cheerful atmosphere.. This made me feel like a true part of the team.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and allows flexible working hours when needed. Additionally, being able to join internal training sessions is a great advantage.\nAdditional Questions – Answers What were you most satisfied with during the internship?\nWhat I was most satisfied with was being given real tasks and receiving helpful feedback that allowed me to improve significantly.\nWhat do you think the company should improve for future interns?\nThe company could organize more onboarding workshops and technical training in the first week so new interns can adapt quickly.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes. Because the learning environment is great, mentors are supportive, and interns are given real opportunities to handle practical tasks instead of minor or unrelated work.\nSuggestions \u0026amp; Expectations – Answers Do you have any suggestions to improve the internship experience?\nThere should be more weekly check-in sessions between mentors and interns to track progress and provide clearer guidance.\nWould you like to continue this program in the future?\nYes, because this internship has provided me with valuable knowledge and motivation to grow further.\nAny additional comments (free sharing):\nI’m truly grateful for this opportunity. The internship has helped me grow in both technical skills and soft skills. Thank you to everyone on the team for your support.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Enhance e-commerce platform with user authentication and authorization. Implement shopping cart and order management functionality. Set up database optimization and security measures. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Implement JWT-based authentication system - Create user registration and login endpoints - Set up password encryption and validation 20/10/2025 20/10/2025 Spring Security Documentation 2 - Design and implement role-based access control - Create admin and customer user roles - Secure API endpoints with proper authorization 21/10/2025 21/10/2025 Spring Security, JWT Documentation 3 - Implement shopping cart functionality - Create cart session management - Handle add/remove/update cart items 22/10/2025 22/10/2025 Redis, Spring Session Documentation 4 - Build order management system - Create order placement and tracking - Implement order status workflow 23/10/2025 23/10/2025 Spring Boot, JPA Documentation 5 - Set up AWS RDS for production database - Configure database security groups - Implement database backup and monitoring 24/10/2025 24/10/2025 AWS RDS, CloudWatch Documentation Week 7 Achievements: Successfully implemented JWT-based authentication system with secure token management. Built comprehensive role-based access control for different user types. Created efficient shopping cart system with session management. Developed complete order management workflow from placement to fulfillment. Migrated database to AWS RDS with proper security configurations. Implemented automated backup strategies for data protection. Set up database monitoring and alerting through CloudWatch. Enhanced API security with proper authentication and authorization layers. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Implement payment integration and order processing system. Set up email notification service and real-time features. Optimize application performance and implement caching strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Integrate payment gateway (Stripe/PayPal) - Implement secure payment processing - Handle payment confirmation and failure scenarios 27/10/2025 27/10/2025 Stripe API, PayPal SDK Documentation 2 - Set up AWS SES for email notifications - Create email templates for orders - Implement order confirmation emails 28/10/2025 28/10/2025 AWS SES Documentation 3 - Implement real-time notifications using WebSocket - Set up order status updates - Create admin notification system 29/10/2025 29/10/2025 WebSocket, Spring WebSocket Documentation 4 - Set up Redis for caching - Implement product catalog caching - Add session caching for better performance 30/10/2025 30/10/2025 Redis, Spring Cache Documentation 5 - Optimize database queries - Implement pagination and filtering - Add search functionality with Elasticsearch 31/10/2025 31/10/2025 JPA, Elasticsearch Documentation Week 8 Achievements: Successfully integrated secure payment processing with multiple payment methods. Implemented comprehensive email notification system using AWS SES. Built real-time notification system for better user experience. Set up Redis caching to improve application performance significantly. Optimized database operations and implemented efficient search functionality. Created robust error handling for payment and order processing. Enhanced system scalability with proper caching strategies. Implemented monitoring for payment transactions and system performance. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.9-week9/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Continue learning and familiarizing yourself with AWS services Learn and implement projects Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Containerization with Docker - Container Orchestration with Amazon ECS - Container Orchestration with Amazon ECS 27/10/2025 27/10/2025 3 - CI/CD Pipeline with AWS CodePipeline - Automated Deployments with AWS CodePipeline - DevOps with AWS CodePipeline 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hybrid Storage with AWS Storage Gateway - Windows File Storage with Amazon FSx 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Windows File Storage with Amazon FSx - Building Advanced Applications with Amazon DynamoDB 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Workflow Orchestration with AWS Step Functions - Storage Performance Workshop 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Containerization with Docker\nBuild and push Docker images for backend + frontend, deploy on EC2.\nCreate Docker network and launch containers, test applications via public IP.\nUse Docker Compose to manage stacks and redeploy the entire system quickly.\nContainer Orchestration with Amazon ECS\nUnderstand Amazon ECS architecture, create and manage ECS clusters.\nDefine ECS tasks and services, configure Application Load Balancer for load balancing.\nDeploy containerized applications on ECS, test deployment and clean up resources after completion.\nContainer Orchestration with Amazon ECS\nBuild containerized Spring Boot (Java 21) microservices and deploy to ECS Fargate.\nUse AWS CDK to create VPC + NAT Gateway, ECS Cluster, Load Balancer, and API Gateway.\nCreate DynamoDB using CDK to serve as a backend storage for service “products”.\nInstrument ECS service with AWS X‑Ray (add sidecar, declare Inspector, test trace).\nClean up resources after completing the workshop.\nCI/CD Pipeline with AWS CodePipeline\nSet up automated CI/CD for containers on Amazon ECS, using GitLab, GitHub or CodeBuild.\nDeploy integration workflow (build → deploy) to ECS Service, ensuring automatic application updates.\nMonitor containers with Container Insights + collect logs via FireLens, ensuring effective observation and debugging.\nClean up AWS resources after completion to optimize costs.\nAutomated Deployments with AWS CodePipeline\nSet up a CodePipeline pipeline integrating CodeCommit → CodeBuild → CodeDeploy to deploy Node.js applications to EC2.\nConfigure CodeDeploy Agent on EC2 (via Session Manager or User Data).\nDeploy the application in-place to EC2 instances, test after deployment.\nSet up infrastructure (VPC, Security Group, RDS if needed) to serve the pipeline.\nClean up AWS resources after completing the lab to save costs.\nDevOps with AWS CodePipeline\nCreate an IAM Role for CodeBuild to interact with the EKS cluster.\nConfigure aws-auth to allow CodeBuild to use kubectl with permissions on the cluster.\nSet up a CodePipeline pipeline using CloudFormation to automatically build + deploy sample services to EKS.\nTrigger new releases when changing code on GitHub → automatically update services on EKS.\nClean up all resources (deployment, stack, bucket) after the lab to avoid additional costs.\nHybrid Storage with AWS Storage Gateway\nCreate S3 bucket + initialize EC2 to use as Storage Gateway.\nConfigure AWS Storage Gateway, create SMB/NFS file share and mount on on-premises machine.\nSet up SMB access (guest) to share files from on-prem machine to S3.\nTest by creating files on mounted drive → files are synchronized to S3.\nClean up resources: delete Gateway, EC2, S3 bucket after completion.\nWindows File Storage with Amazon FSx\nDeploy FSx Windows File Server using CloudFormation in VPC.\nCreate and map SMB file shares on EC2 Windows, test with sample data.\nEnable shadow copies to save previous file versions \u0026amp; configure backup storage.\nEnable Continuous Access share to support HA for applications such as SQL Server.\nAdjust storage capacity and throughput according to needs; clean up resources after doing lab\nBuilding Advanced Applications with Amazon DynamoDB\nDo hands-on DynamoDB lab to understand the basics of querying, Streams, Global Tables and NoSQL data model.\nBuild advanced “design patterns” for DynamoDB (partitioning, GSI, adjacency list).\nDeploy global serverless applications with DynamoDB Global Tables.\nWorkflow Orchestration with AWS Step Functions\nDesign and implement state machine orchestration using Lambda: including task, branching (Choice) and parallel (Parallel).\nConfigure retry \u0026amp; catch errors when Lambda encounters problems for more sustainable workflow.\nUse token callback (waitForTaskToken) to allow pause/resume workflow.\nSet up debugging / logging, visualize workflow and clean up resources after lab completion Storage Performance Workshop\nInitialize resources via CloudFormation (EC2, Security Group) to run performance lab.\nOptimize S3 throughput, use parallel prefix, sync command, and upload multiple small files to increase TPS.\nOptimize EFS performance: configure IOPS, I/O size, multi-threading and choose appropriate performance mode.\nClean up resources after lab: delete S3 bucket, terminate EC2, delete CloudFormation stack.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn more about aws services Continue working on the project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Serverless Backend with Lambda, S3, and DynamoDB - Frontend Development for Serverless APIs 03/11/2025 03/11/2025 3 - Deployment Automation with AWS SAM - User Authentication with Amazon Cognito 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Custom Domains and SSL for Serverless Applications - Event Processing with SQS and SNS 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - CI/CD for Serverless Applications - Monitoring Serverless Applications - Building GraphQL APIs with AWS AppSync 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Building Serverless APIs - Serverless Chat Application 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10Achievements: Frontend Development for Serverless APIs Build a web interface (frontend) to call API created by API Gateway + Lambda.\nDeploy front-end: can host static site (HTML/CSS/JS) connecting to serverless backend.\nDeploy Lambda function to handle request logic from API Gateway.\nConfigure API Gateway as endpoint for frontend: define route, HTTP method,…\nTest API with Postman: ensure API Gateway + Lambda runs correctly before frontend calls.\nTest frontend: web app calls API Gateway, receives results from Lambda and displays.\nDeployment Automation with AWS SAM Use AWS SAM (Serverless Application Model) to define serverless application via YAML file, then SAM will convert to CloudFormation when deploying.\nCreate and deploy front-end, Lambda function and API Gateway with SAM.\nTest API with Postman and frontend after deploying.\nUser Authentication with Amazon Cognito Create a Cognito User Pool to manage registration, login, email verification, password change, password reset.\nUse Identity Pool if you need to grant AWS Credentials (temporarily) to users to access services such as S3, DynamoDB.\nCreate API and Lambda: after the user authenticates via Cognito, Lambda handles the API request.\nFrontend: build login/registration UI, call Cognito API to log in and get token; then send token to API Gateway/Lambda.\nCustom Domains \u0026amp; SSL for Serverless Apps Use API Gateway to configure a custom domain name (“custom domain”) instead of the default hostname of API Gateway.\nUse Amazon Certificate Manager (ACM) to create or import SSL/TLS certificate for the domain, ensuring HTTPS.\nSelect custom domain type: edge-optimized (using CloudFront) or regional.\nConfigure Base Path Mapping to map domain + path to API Gateway stages.\nCreate DNS record (Route 53 or other DNS) to point domain to CloudFront distribution or endpoint provided by API Gateway.\nSSL and security: you can choose TLS security policy, ACM will manage certificate renewal. AWS Docs\n**Event Processing with SQS \u0026amp; SNS ** When users place an order, API sends messages to SQS queue to ensure “queue” to process the order.\nAt the same time, SNS is used to notify admin every time there is a new order.\nCreate Lambdas:\ncheckout_order to receive orders from API and push messages to SQS + publish SNS.\nhandle_order for admin to process orders: read from SQS, write orders to DynamoDB.\ndelete_order for admin to delete order: delete message in SQS.\norder_management for admin to view order list (using DynamoDB).\nArchitecture uses SQS for durability and asynchronous processing, SNS for fan-out notifications.\nCI/CD for Serverless Apps Use AWS SAM Pipelines to automate the process of building, packaging and deploying serverless applications.\nInitialize CI/CD pipeline: sam pipeline init to create pipeline configuration for AWS CodePipeline\nPipeline includes many stages: Source (Git), Build (using SAM CLI), Deploy (CloudFormation/SAM).\nUse build container image provided by AWS SAM to compile serverless app, helping reduce the effort of creating separate CI image.\nPipeline follows AWS best-practice pattern: supports multi-account, multi-region.\nMonitoring Serverless Applications Use CloudWatch Logs to debug Lambda: log invocations, errors, etc.\nCreate custom metrics in CloudWatch to monitor business or performance metrics.\nConfigure CloudWatch Alarms based on metrics to alert when there is an abnormal situation.\nUse AWS X-Ray to trace requests: draw service maps, see the path of requests and find bottlenecks.\n###Building GraphQL APIs with AWS AppSync\nUse AWS AppSync to create serverless GraphQL APIs, combined with AWS data sources such as DynamoDB, Lambda or Aurora Serverless.\nDefine GraphQL schema: type, query, mutation, subscription (if real-time required). AWS Docs\nConfigure resolvers:\nUse VTL mapping template to convert GraphQL requests into data source query operations.\nOr use Direct Lambda Resolver: Lambda handles GraphQL logic, avoiding the need to write VTL.\nIf using a relational database (Aurora Serverless), AppSync can connect via Data API to execute SQL commands via GraphQL mutation/query.\nAPI access can be managed in the following ways: AWS IAM, Cognito User Pools,… (authentication + authorization).\nBuilding Serverless APIs (Serverless with Lambda, API Gateway and SAM) Backend architecture uses Lambda, API Gateway, DynamoDB, S3, and Cognito.\nFrontend is a JavaScript application (Vue.js) hosted on S3 / Amplify.\nUse AWS SAM to define serverless resources and deploy backend (Lambda + API Gateway + DynamoDB)\nLambda reads / writes data from DynamoDB: for example, Lambda scans DynamoDB table to return list of “parks” (rides, attractions).\nDeploy API Gateway for frontend to call API; Lambda handles request logic.\nIn realtime: Lambda subscribes to SNS topic, saves information to DynamoDB, and forwards payload to IoT Core so frontend can receive real-time data.\nServerless Chat Application Build a serverless real-time chat application using API Gateway WebSocket, Lambda, and DynamoDB.\nWhen a client connects (“$connect”), Lambda stores the connection ID in DynamoDB.\nWhen a client disconnects (“$disconnect”), Lambda deletes the connection ID from DynamoDB.\nWhen a client sends a message (“sendmessage” route), Lambda reads the connection IDs from DynamoDB and uses the API Gateway Management API to broadcast the message to all connected clients.\n"
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Execute production deployment and go-live activities. Implement production monitoring and support procedures. Conduct user acceptance testing and bug fixes. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Execute production deployment to AWS - Configure production environment variables - Run database migration and data seeding 17/11/2025 17/11/2025 Deployment Checklist, AWS Best Practices 2 - Conduct user acceptance testing with stakeholders - Document and prioritize identified issues - Create bug tracking and resolution workflow 18/11/2025 18/11/2025 UAT Test Plans, JIRA/Bug Tracking Tools 3 - Implement critical bug fixes and patches - Deploy hotfixes to production environment - Verify system stability and performance 19/11/2025 19/11/2025 Bug Fixing Procedures 4 - Set up production monitoring dashboards - Configure alerting for critical system metrics - Establish on-call support procedures 20/11/2025 20/11/2025 CloudWatch, PagerDuty Documentation 5 - Conduct post-deployment review - Document lessons learned - Create operational runbooks and troubleshooting guides 21/11/2025 21/11/2025 Operations Documentation Week 11 Achievements: Successfully deployed clothing e-commerce platform to production. Completed user acceptance testing with stakeholder approval. Resolved all critical and high-priority bugs identified during testing. Established comprehensive production monitoring and alerting system. Created detailed operational procedures and support documentation. Implemented automated backup and recovery procedures. Set up 24/7 monitoring with appropriate escalation procedures. Achieved production stability with minimal downtime during deployment. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Complete final project documentation and knowledge transfer. Conduct project retrospective and performance evaluation. Plan future enhancements and maintenance activities. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Complete comprehensive project documentation - Create technical architecture documentation - Document API specifications and user guides 24/11/2025 24/11/2025 Technical Writing Best Practices 2 - Conduct knowledge transfer sessions with team - Create video tutorials for system operations - Document troubleshooting procedures 25/11/2025 25/11/2025 Knowledge Transfer Templates 3 - Perform final system performance analysis - Generate usage reports and metrics - Create recommendations for future improvements 26/11/2025 26/11/2025 Performance Analysis Tools 4 - Conduct project retrospective meeting - Document lessons learned and best practices - Create improvement recommendations 27/11/2025 27/11/2025 Retrospective Meeting Templates 5 - Finalize all project deliverables - Submit final project report - Prepare presentation for stakeholders 28/11/2025 28/11/2025 Project Management Documentation Week 12 Achievements: Completed comprehensive project documentation covering all aspects of the system. Successfully conducted knowledge transfer to ensure smooth handover. Generated detailed performance analysis and optimization recommendations. Documented all lessons learned and best practices for future reference. Delivered complete clothing e-commerce platform with AWS integration. Achieved all project objectives within timeline and budget constraints. Established maintenance procedures and future enhancement roadmap. Received positive feedback from stakeholders on project delivery and quality. "
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyentanxuan.github.io/fcj/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]