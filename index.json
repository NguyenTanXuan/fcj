[{"uri":"https://nguyentanxuan.github.io/fcj/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Tan Xuan\nPhone Number: 0857291939\nEmail: tanxuan31052004@gmail.com\nUniversity: FPT University Ho Chi Minh city\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 14/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://nguyentanxuan.github.io/fcj/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Lessons Learned: AWS Cloud Mastery Series #1 — AI/ML/GenAI on AWS 1. Event Purpose Provide an overview of AI/ML in Vietnam.\nPresent AWS\u0026rsquo;s core AI/ML services, especially Amazon SageMaker.\nClarify Generative AI applications through Amazon Bedrock and techniques such as RAG, Prompt Engineering.\n2. Main Content Morning: Overview of AWS AI/ML Services Introduce AI/ML context and workshop objectives.\nPresent Amazon SageMaker and the full ML process: Data Prep → Labeling → Training → Tuning → Deployment.\nMLOps model and integration into the development process.\nLive demo of SageMaker Studio.\nAfternoon: Generative AI with Amazon Bedrock Introduction to Foundation Models and selection criteria (Claude, Llama, Titan\u0026hellip;). Advanced Prompt Engineering techniques: Chain-of-Thought (CoT), Few-shot learning. Explain RAG architecture and integration with Knowledge Base. Bedrock Agents and how to build multi-step workflows. Guardrails and content control mechanism. Demo of building GenAI chatbot on Bedrock. 3. Knowledge extracted Understand SageMaker as a full ML platform serving the entire model lifecycle. Understand the role of Bedrock, the characteristics of each Foundation Model and core GenAI techniques. Be able to apply RAG and Bedrock Agents to improve chatbot in Travel-Guided project. Understand the actual deployment process through demos. 4. Hands-on experience Demos demonstrate Bedrock\u0026rsquo;s fast prototyping speed and strong integration capabilities. Opportunities to interact directly with AWS experts and the AI/ML community in Vietnam. "},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn basic AWS, get familiar and practice with Console \u0026amp; CLI\nWeek 3: Learn AWS services, EC2, and practice deploying resources\nWeek 4: Review knowledge, prepare input for system architecture design\nWeek 5: Begin designing system architecture and integrating AWS services\nWeek 6: Backend CRUD APIs for products and orders, initial MSSQL database design\nWeek 7: User management module: registration, login, and role-based access\nWeek 8: Shopping cart and payment module development; frontend API integration\nWeek 9: Progress with great strides of the Sales Web Project\nWeek 10: Understand and apply AWS to the Sales Web Project\nWeek 11: Optimize APIs, implement search, filter, sort features; documentation\nWeek 12: Finalize backend, prepare demo and deployment documentation\n"},{"uri":"https://nguyentanxuan.github.io/fcj/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"OpenAI Open-Weight Models Now Available on AWS OpenAI’s open-weight models are now available on AWS, giving organizations and developers the ability to integrate advanced AI with greater flexibility, scalability, and control. These models are designed for text generation, reasoning, programming, and scientific workloads, with support for context lengths up to 128K tokens and adjustable reasoning levels.\nThis blog post outlines the availability of OpenAI’s new models, the architectural choices behind their AWS integration, and how developers can get started using Amazon Bedrock and Amazon SageMaker JumpStart.\nArchitecture Guidance The new models — gpt-oss-120b and gpt-oss-20b — are available through both Amazon Bedrock and Amazon SageMaker JumpStart:\nAmazon Bedrock: Provides serverless, API-driven access to models via InvokeModel or Converse. SageMaker JumpStart: Enables fine-tuning, scalable deployment, and production integration using either the console or the Python SDK. Solution architecture highlights include:\nDirect model access via Bedrock endpoints Managed deployment options through SageMaker Support for multi-service workflows and event-driven architectures While the term open-weight models is broad, some key characteristics include:\nHigh flexibility with custom deployment options Transparency through reasoning traces for interpretability Designed for embedding into event-driven and agentic workflows Compatibility with OpenAI SDKs and AWS-native services When evaluating deployment options, consider:\nIntrinsic factors: context size, performance, reasoning quality Extrinsic factors: scalability, integration requirements, reusability Human factors: developer productivity, operational ownership Technology Choices and Communication Scope Integration Scope AWS Services / Patterns to Consider Model access \u0026amp; testing Amazon Bedrock (chat playground, APIs) Fine-tuning \u0026amp; deployment Amazon SageMaker JumpStart Event-driven workflows Amazon EventBridge, AWS Lambda, Bedrock AgentCore Tool-augmented AI agents Strands Agents with Bedrock-integrated models Hub-and-Spoke Analogy for Model Integration Similar to a pub/sub hub used in microservices, AWS enables a hub-and-spoke approach for AI integration:\nModels are accessed via Bedrock endpoints Interactions are standardized through OpenAI-compatible APIs Developers can switch between different foundation models without rewriting application logic Trade-off: careful coordination and monitoring are required to manage multiple models and reasoning levels effectively.\nCore Model Access The foundation for using OpenAI models on AWS includes:\nAmazon Bedrock for serverless inference Amazon SageMaker JumpStart for managed deployment and fine-tuning OpenAI SDK compatibility for seamless integration Reasoning trace support for improved transparency These models are production-ready and optimized for integration with existing AWS workloads.\nDeveloper Entry Point Access models through the Amazon Bedrock console: request model access and start testing in the chat playground Authenticate via AWS IAM policies Use the OpenAI SDK by pointing OPENAI_BASE_URL to the Bedrock runtime endpoint Example Python code:\nfrom openai import OpenAI client = OpenAI() response = client.chat.completions.create( messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me the square root of 42 ^ 3\u0026#34;}], model=\u0026#34;openai.gpt-oss-120b-1:0\u0026#34;, stream=True ) for item in response: print(item) "},{"uri":"https://nguyentanxuan.github.io/fcj/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS FOR SAP Introduction Migrating SAP landscapes to AWS gives enterprises the opportunity to modernize their infrastructure and leverage the elasticity of the cloud. Businesses can scale compute, memory, and storage within minutes and automate provisioning to reduce operational errors.\nHowever, applying auto scaling to SAP Application Servers introduces complexity. Traditionally, organizations over-provision resources to avoid system slowdowns during peak load periods—resulting in paying for maximum capacity even when it’s not needed.\nNative AWS components such as Auto Scaling groups, Target Groups, and Launch Templates do not fully address SAP’s architectural requirements. To solve this challenge, AWS Professional Services developed the SAP Auto Scaling solution, enabling horizontal scaling of SAP Application Servers and reducing infrastructure costs by up to 50%.\nBusiness Case for Auto Scaling The SAP Auto Scaling solution is designed to address real operational challenges, such as:\nEnsuring SAP environments dynamically adapt to workload demand—scaling out during peak load and scaling in when demand drops. Allowing administrators to proactively activate or deactivate servers for maintenance or system upgrades. Reducing operational overhead by automating routine infrastructure tasks. This solution provides the greatest value for customers who:\nExperience uneven workload cycles (e.g., lower usage during evenings or weekends). Are looking to reduce EC2 spend during off-peak hours. Operate production environments with three or more SAP Application Servers. Prefer flexible cost models without long-term commitments. Key Benefits Modernizes SAP environments by applying native cloud elasticity. Achieves significant EC2 cost savings—up to 50% compared to running On-Demand instances continuously. Supports predictable scaling patterns aligned with business demand. How the Solution Works SAP Auto Scaling automatically manages the active or inactive state of SAP Application Servers. It continuously monitors the usage of Dialog Work Processes on the ASCS (ABAP SAP Central Services) instance to determine when to scale.\nCore Functionality When load increases, the system starts a preconfigured EC2 instance and automatically launches its SAP services. Scale decisions (up or down) are based exclusively on Dialog Work Process utilization on the ASCS. Before a scale-down event, all active SAP GUI users receive an on-screen notification to save their work and reconnect. SAP Soft Shutdown ensures all batch jobs are completed before a server is turned off. Administrators can schedule start times for Application Servers. “No-scale windows” can be defined to block scaling during critical business hours. Critical servers can be marked as “always on.” Email notifications are sent for all scale events. Scale-Up Decision Tree The system follows a structured\n"},{"uri":"https://nguyentanxuan.github.io/fcj/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Measuring the Value of Learning \u0026amp; Development (L\u0026amp;D) Programs To an L\u0026amp;D leader, the importance of employee learning is obvious. But stakeholders across the organization may not immediately see the connection. Building trust requires proving the return on investment (ROI) of learning initiatives.\nIn this post, we summarize insights from Kristin Thomas (Director, Customer Value Realization at Pluralsight) on how to demonstrate the impact of upskilling and align stakeholders around measurable business value.\nKey Components of Proving L\u0026amp;D Value Kristin highlights two essential questions:\nHave employees gained new skills? Are they applying those skills to mission-critical work? Without visibility into these two factors, it’s impossible to link learning activities to business KPIs such as retention, customer satisfaction, or ticket resolution time.\nThree Steps to Demonstrating L\u0026amp;D ROI Step 1: Understand the Business Context Before launching an upskilling initiative, identify the business problem it aims to solve. Examples include:\nCompliance requirements Faster project timelines Higher product quality Talent retention Workforce productivity Clarifying the end goal helps define which outcomes should be measured.\nStep 2: Design Upskilling for the Future of Work Determine the capabilities your teams must have after training. This creates alignment with business objectives.\nAlign with existing KPIs Study the metrics used by other teams:\nHow is productivity measured? What sales metrics reflect improvement? How do you track voluntary/involuntary turnover? How is process speed measured? Choose 1–3 primary metrics to anchor L\u0026amp;D outcomes.\nEvaluate current skill \u0026amp; confidence levels Compare what employees can do now versus what they need to support future strategy.\nHigher confidence = higher likelihood of applying new skills.\nSupport learning behaviors Enable consistent learning by:\nGiving employees dedicated learning time Explaining relevance and impact Providing tools that support new workflows Step 3: Track Learning Metrics \u0026amp; Learner Preferences Engagement metrics (hours learned, logins) are not enough. Instead, track whether:\nLearning resources match role expectations Skills gained align with business goals Employees find learning relevant and usable Preferred learning methods (classroom, digital, hands-on) are provided Continuous feedback keeps programs aligned with organizational needs.\nStart Small You don’t need a full ROI model immediately. Begin with simple metrics:\nHow many employees upskilled? What skills did they gain? How much improvement occurred? Then expand into deeper performance and business outcomes.\n"},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 • Get acquainted with FCJ members • Read and take note of internship unit rules and regulations • Learn basic AWS lesson and set up coding environment 09/09/2025 09/09/2025 YouTube Playlist 2 • Learn about AWS service categories (Compute, Storage, Networking, Database, …) 10/09/2025 10/09/2025 Cloud Journey 3 • Create AWS Free Tier account • Learn AWS Console \u0026amp; CLI • Practice: create account, install \u0026amp; configure AWS CLI 11/09/2025 11/09/2025 Cloud Journey 4 • Learn basic EC2 concepts (Instance types, AMI, EBS, SSH, Elastic IP) 12/09/2025 12/09/2025 Cloud Journey 5 • Practice with EC2: launch instance, connect via SSH, attach EBS volume 13/09/2025 13/09/2025 Cloud Journey 6 • Review and summarize Week 1 achievements • Prepare notes/questions for Week 2 14/09/2025 14/09/2025 Personal Notes Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://nguyentanxuan.github.io/fcj/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS First Cloud AI Journey – Project Plan Online Shopping Website: Furious Five Fashion (FFF) AWS \u0026amp; AI-Powered E-commerce Website Solution 1. Background and Motivation 1.1 Executive Summary The client is a small-sized business specializing in fashion products for young customers. They aim to build an online clothing e-commerce website using AWS and AI, with the ability to scale flexibly, support long-term growth, and optimize operational costs.\nThe goal of this project is to shift from traditional manual management on physical servers to a flexible, intelligent, and cost-efficient cloud-based model. AWS enables the system to scale at any time, maintain fast access speed, and allow the business to focus on product development instead of infrastructure.\nThe system is designed to support end-to-end e-commerce operations: hosting and distributing web content, managing product and order databases, supporting payments, and monitoring system performance. Everything aims toward stability, security, and long-term scalability.\nThe Furious Five implementation team will accompany the client throughout the process—advising, designing the architecture, and configuring key AWS services such as Lambda, S3, DynamoDB, CloudFront, and Route 53. Beyond building the system, they also help optimize costs, ensure security, and train the internal team to manage the infrastructure effectively.\nThis project is not just a technical plan—it marks an important step in the company’s digital transformation journey.\n1.2 Project Success Criteria To ensure the success of the Furious Five Fashion project, the following clear and measurable criteria must be met, representing both business goals and technical effectiveness:\nSystem Performance The website must maintain response times under 2 seconds for all user actions, even during peak hours.\nAvailability The system must achieve 99.9% uptime, monitored and automatically reported through services like CloudWatch.\nScalability AWS infrastructure must scale automatically when traffic increases by at least 2× without causing service disruption.\nCost Optimization Monthly operating costs must remain under 30% of the projected budget, supported by AWS cost-monitoring tools such as Cost Explorer and Trusted Advisor.\nSecurity No data leaks or unauthorized access. All customer data must be protected by AWS security standards (IAM policies, encryption, HTTPS, etc.).\nDeployment \u0026amp; Operations Infrastructure must be fully deployed within 4 weeks, with complete documentation so the internal team can manage the environment effectively.\nTraining \u0026amp; Knowledge Transfer The internal technical team must be trained to confidently maintain, monitor, and secure the system without depending entirely on external support.\n1.3 Assumptions To ensure alignment and smooth execution of the FFF project, the following assumptions have been made:\nThe team already has access to AWS accounts with required permissions and has basic knowledge of essential AWS services such as Lambda, S3, IAM, and Route 53. Stable Internet connectivity is assumed since all infrastructure runs in the cloud. The team is also aware of basic security and compliance requirements before deployment.\nThe project depends on multiple external factors: stable service availability in the selected AWS region, smooth domain routing via Route 53, and effective collaboration between development teams to ensure the web application operates properly in the cloud environment.\nThe project is part of an internship, so the budget is limited—favoring free-tier usage and low-cost service configurations. Due to limited experience and tight timelines, the chosen architecture remains simple and practical.\nPotential risks include IAM misconfigurations, accidental overspending due to unused resources, AWS regional outages, service incompatibilities, or limited expertise in troubleshooting cloud systems.\nDespite these risks, the project is built on clear expectations: this is a pilot environment, with layered monitoring, backup, and cost-management strategies in place. Every challenge is considered an opportunity to learn and grow in cloud engineering.\n2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The following architecture is designed for FFF, deployed in AWS Region Singapore (ap-southeast-1). It emphasizes flexibility, security, automation, scalability, and simplicity—appropriate for an internship-level project while following AWS best practices.\nThe system follows a multi-layer design consisting of six key components:\nFrontend \u0026amp; Security Layer Users access the website through Route 53. Incoming traffic is protected with AWS WAF and optimized via CloudFront CDN. Source code is managed and deployed through GitLab CI/CD using CloudFormation templates.\nAPI \u0026amp; Compute Layer API Gateway routes all requests to AWS Lambda, which handles application logic. Cognito manages authentication and access control.\nStorage Layer Two S3 buckets store static content (StaticData) and user uploads.\nData Layer DynamoDB stores product metadata and unstructured data. IAM ensures secure interactions between components.\nAI Layer Amazon Rekognition and Amazon Bedrock power image processing and generative AI features.\nObservability \u0026amp; Security Layer CloudWatch, SNS, and SES provide monitoring, alerting, and system notifications.\n2.2 Technical Implementation Plan Infrastructure will be managed and deployed using Infrastructure as Code (IaC) with AWS CloudFormation to ensure repeatability, stability, and ease of maintenance. Key AWS components—S3, Lambda, API Gateway, DynamoDB, Cognito, and CloudWatch—will be defined entirely through CloudFormation templates stored in GitLab for version control and rollback capability.\nSensitive configurations such as IAM permissions or WAF rules require approval before deployment and follow the internal governance process with review and validation.\nAll critical system paths—from authentication to data processing—are covered by automated and manual test cases to ensure stability, security, and scalability.\nThis technical plan enables the FFF team to deploy and manage a professional cloud environment, learning real DevOps and AWS best practices.\n2.3 Project Plan The project follows Agile Scrum over 3 months, divided into 4 sprints.\nSprint Structure\nSprint Planning\nSetup AWS foundational services (S3, Route 53, IAM)\nConfigure security (WAF, CloudFront)\nIntegrate backend (Lambda, API Gateway, DynamoDB)\nTesting, optimization, and demo preparation\nDaily Stand-up 30-minute updates to address blockers and track status.\nSprint Review Review deliverables, demo on real AWS environment, fix issues.\nRetrospective Improve DevOps workflows and automation pipeline.\nTeam Roles\nProduct Owner: Business alignment, backlog prioritization\nScrum Master: Coordination, Agile process enforcement\nDevOps/Technical Team: Backend, infrastructure, CI/CD\nMentor / AWS Partner: Architecture validation, AI testing, cost \u0026amp; security review\nCommunication Rhythm\nDaily Stand-ups (23:00)\nWeekly Sync\nEnd-of-Sprint Demo\nKnowledge Transfer After the final sprint, the technical team will deliver hands-on training on operations, monitoring (Budgets, CloudWatch), scaling, and recovery procedures.\n2.4 Security Considerations Access Management MFA for admin users; IAM roles with least privilege; auditing through CloudTrail.\nInfrastructure Security\nEven without a dedicated VPC, services are restricted using resource policies; all public endpoints use HTTPS.\nData Protection\nS3 and DynamoDB encryption; TLS data transfer; manual periodic backups.\nDetection \u0026amp; Monitoring\nCloudTrail, Config, and CloudWatch for visibility; GuardDuty for threat detection.\nIncident Response\nClear incident workflows with log collection, analysis, and periodic simulations.\n3. PROJECT ACTIVITIES \u0026amp; DELIVERABLES 3.1 Activities \u0026amp; Deliverables Table Phase Timeline Activities Deliverables Effort(day) Infrastructure Setup Week 1 – 2 Requirements gathering, architecture design, AWS configuration (S3, CloudFront, API, Lambda, DynamoDB, Cognito), GitLab CI/CD setup Completed AWS Architecture, Ready Infrastructure, Active CI/CD 10 Frontend Development Week 3–5 UI/UX design, FE pages (Home, Catalog, Product Detail, Cart, Checkout), API integration Completed FE (Dev), Frontend connected to API 15 Backend \u0026amp; Database Week 6–9 Lambda APIs, DynamoDB setup, order/user/product logic, Cognito IAM setup Stable API, validated data flow, full FE-BE integration 20 Testing \u0026amp; Validation Week 10–11 Functional, security, performance testing, integration testing Test Report, Validated System 5 Production Launch Week 12 Deploy to production, domain \u0026amp; SSL setup, training \u0026amp; handover Live FFF Website, Documentation Package 5 3.2 Out of Scope Mobile applications (iOS/Android)\nReal inventory/logistics integration\nAdvanced admin dashboards\nCRM/ERP integrations\nAdvanced AWS security services\nReal payment gateway integration\nMultilanguage \u0026amp; multicurrency\n3.3 Go-Live Roadmap Phase 1 – POC Basic FE, S3 hosting, API integration, sample data storage, CloudFront optimization.\nPhase 2 – UAT Cognito auth, sandbox payment, CloudWatch monitoring, internal user testing.\nPhase 3 – Production Deployment Route 53 domain setup, SSL via ACM, WAF protection, CloudFront refinement.\nPhase 4 – Stabilization \u0026amp; Optimization Cost optimization, performance improvements, backup strategy, documentation updates.\n4. AWS COST ESTIMATION Estimated monthly cost: $30–35 USD\nRoute 53 : $1.00 AWS WAF : $5.00 CloudFront: $3.90 S3 (StaticData) : $0.50 S3 (Uploads): $0.75 S3 (Bucket): $0.75 AWS Lambda: $0.25 API Gateway: $3.50 Amazon Bedrock: $3.00 DynamoDB: $1.00 IAM: Free CloudWatch: $2.00 SNS: $0.10 SES: $0.20 CloudFormation: Free GitLab CI/CD : $3.00 WS Config / Setup \u0026amp; Test migration tools $5.00 (1 lần) Estimated monthly total cost: ~ $30.00 – $35.00 USD Cost assumptions:\nRegion: Singapore\n500–1000 users/month\nLow traffic\nData \u0026lt; 100GB\nFree Tier active for 12 months\nCost optimizations recommended:\nS3 Intelligent-Tiering\nCloudWatch log retention 14–30 days\nAWS Budgets alert at $40\nConsider Lambda Savings Plan for long-term workloads\n5. Project Team Project Stakeholders Name: Van Hoang Kha Title: Support Teams Description: is the Executive support person responsible for overall supervision of the FCJ internship program\nEmail/Contact information: Khab9thd@gmail.com\nPartner Project Team (Furious Five Internship Team)\nName: Duong Minh Duc Title: Project Team Leader\nDescription: Manage progress, coordinate work between the team and mentor, Manage AWS infrastructure deployment (S3, Lambda, IAM)\nEmail/Contact information: ducdmse182938@fpt.edu.vn\nName: Quach Nguyen Chi Hung\nTitle: Member\nDescription: In charge of UI/UX and user interface\nEmail/Contact information: bacon3632@gmail.com Name: Nguyen Tan Xuan\nTitle: Member\nDescription: Responsible for Backend and server logic processing\nEmail/Contact information: xuanntse184074@fpt.edu.vn\nName: Nguyen Hai Dang\nTitle: Member\nDescription: Manage AWS infrastructure deployment (S3, Lambda, IAM) and AI chat bot integration\nEmail/Contact information: dangnhse184292@fpt.edu.vn\nName: Pham Le Huy Hoang\nTitle: Member\nDescription: Testing, quality assurance and GitLab CI/CD integration, and AI chat bot integration\nEmail/Contact information: hoangplhse182670@fpt.edu.vn\nContact Complaints / Escalation Project Project\nName: Duong Minh Duc\nTitle: Project Team Leader\nDescription: Represent the internship team to contact the mentor and sponsor directly\nEmail/Contact information: ducdmse182938@fpt.edu.vn\n6. RESOURCES \u0026amp; COST ESTIMATES Resources Roles Responsibilities Rate (USD)/Hour Solution Architect(1) Design overall solutions, ensure technical feasibility, select appropriate AWS services 35 Cloud Engineer(2) Implement AWS infrastructure, configure services (S3, IAM\u0026hellip;), test and optimize systems 20 Project Manager (1) Monitor progress, coordinate teams, manage project scope and risks. 15 Support / Documentation (1) Prepare handover documents, user manuals, and final reports. 10 Estimate costs by project phase Project phase Solution Architect (hrs) 2 Engineers (hrs) Project Manager (hrs) Project Management / Support (hrs) Total Hours Survey \u0026amp; Solution Design 53 40 13 13 119 Implementation \u0026amp; Testing 67 160 21 19 267 Support / Documentation 27 53 21 19 120 Total Hours 147 253 55 51 506 Total Amount $5145 $5060 $825 $510 $11540 Cost Contribution Allocation Party Contribution (USD) % Contribution Customers 4616 40% Partners (Furious Five) 2308 20% AWS 4616 40% 7.\tACCEPTANCE Since this project is currently at the presentation stage and has not yet been formally evaluated by a customer, the following acceptance process is proposed for future delivery phases:\n7.1 Acceptance Criteria (Proposed) A deliverable will be considered acceptable when it meets the following criteria:\nFunctional features work as specified (authentication, recipe management, social features, AI functions). All APIs respond correctly and integrate with AWS services (Lambda, API Gateway, RDS, S3). Security requirements are met (JWT verification, HTTPS, access control, data encryption). UI works as expected on supported devices. No critical errors appear during test execution. 7.2 ACCEPTANCE PROCESS Review period: 8 business days for evaluation and testing. If accepted → Deliverable is signed off. If issues are found → A rejection notice will be issued with feedback. Fixes will be applied and a revised version will be resubmitted for review. If no response is received by the end of the review period → Deliverable is deemed accepted. After completing each milestone, the team submits the deliverables and documentation.\nTải file .docx "},{"uri":"https://nguyentanxuan.github.io/fcj/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Lessons Learned: AWS Cloud Mastery Series #2 — DevOps on AWS 1. Event Objectives Enhance understanding of DevOps mindset and culture. Clarify AWS CI/CD services: CodeCommit, CodeBuild, CodeDeploy, CodePipeline. Gain knowledge of Infrastructure as Code with CloudFormation and AWS CDK. Learn about Container services (ECS, EKS, App Runner) and observability systems (CloudWatch, X-Ray). 2. Main Contents Morning: CI/CD Pipeline \u0026amp; Infrastructure as Code DevOps Mindset and DORA metrics (Deployment Frequency, MTTR…).\nCI/CD System:\nSource: CodeCommit and Git strategy.\nBuild/Test: CodeBuild.\nDeployment: CodeDeploy with Blue/Green, Canary, Rolling.\nPipeline: automation with CodePipeline.\nDemo of a complete CI/CD pipeline.\nInfrastructure as Code:\nCloudFormation: templates, stacks, drift detection.\nAWS CDK: constructs, reusable patterns, multi-language.\nDemo and discussion of appropriate IaC options.\nAfternoon: Container \u0026amp; Observability Container Services: Docker: basic containerization knowledge. Amazon ECR: storage, scanning and lifecycle image management. Amazon ECS/EKS: deployment, scaling, orchestration. App Runner: PaaS for containers. Monitoring \u0026amp; Observability: CloudWatch: metrics, logs, alarms, dashboards. AWS X-Ray: distributed tracing for microservices. Best Practices: Feature flags, A/B testing. Incident management and postmortems. Demo and case study: evaluate deployment strategy for microservices. 3. Knowledge learned Understand DevOps thinking and the meaning of DORA metrics. Understand how to coordinate between Code* services to build a complete CI/CD. Have a solid foundation in IaC (CloudFormation, CDK) to optimize multi-stack architecture. Clearly see the importance of Monitoring/Tracing through CloudWatch and X-Ray. Understand container architecture orientation with ECS/EKS for large-scale systems. Practical CI/CD \u0026amp; IaC demo, immediately applicable to projects. 4. Experience the event Clear, practical content. Demo and case study support easy visualization of the deployment process. Opportunity to connect directly with experts and the DevOps community in Vietnam. "},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/15/2025 09/15/2025 3 - Learn about AWS Backup: centralized backup management + How Backup Plan works + Backup schedule \u0026amp; lifecycle + Tagging \u0026amp; monitoring backups 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn AWS Compute basics: + EC2 overview + Instance families \u0026amp; types + AMI (Amazon Machine Image) + Purchasing options (On-demand, Reserved, Spot) + Nitro system \u0026amp; hypervisor 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Dive into container services: + ECS (Elastic Container Service) + EKS (Elastic Kubernetes Service) + Fargate (serverless container) + ECR (Elastic Container Registry) 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn optimization \u0026amp; management: + Compute Optimizer + Auto Scaling + Hibernation - Practice: + Launch EC2 instance \u0026amp; connect + Try backup setup + Explore scaling options 09/19/2025 09/20/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood the role of AWS Backup as a centralized management service, while actual backup capability depends on each AWS service. Learned about EC2 instances, their operating systems, architectures (x86, ARM/Graviton), purchasing models, AMIs, and Nitro Hypervisor. Explored container services: ECS (simple), EKS (Kubernetes-based), Fargate (serverless), and ECR for storing container images. Understood optimization tools and features: Compute Optimizer, Auto Scaling, and Hibernation. Practiced with EC2: launching, connecting via SSH, attaching volumes, and trying backup configuration. Acquired a clear overview of how to choose compute and backup solutions on AWS based on cost, performance, and management needs. "},{"uri":"https://nguyentanxuan.github.io/fcj/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Lessons Learned: AWS Cloud Mastery Series #3 — AWS Well-Architected Security Pillar 1. Event Purpose Introduce the role of Security Pillar in AWS Well-Architected Framework. Present 5 pillars of security: IAM, Detection, Infrastructure Protection, Data Protection, Incident Response. Provide best practices and real-world scenarios to protect applications in the Cloud environment. 2. Main Content Pillar 1 — Identity \u0026amp; Access Management (IAM) Principles: Least Privilege, Zero Trust, Defense in Depth. Modern IAM: Avoid long-term credentials, prioritize IAM Roles and managed policies. IAM Identity Center: SSO, Permission Sets, centralized access management. Multi-account security: Service Control Policies and Permission Boundaries. Mini demo: Test IAM policy and simulate access behavior. Pillar 2 — Detection Continuous monitoring with CloudTrail (org-level), GuardDuty, Security Hub. Multi-tier logging: VPC Flow Logs, ALB logs, S3 access logs. Automated alerts via Amazon EventBridge. Pillar 3 — Infrastructure Protection Network security: network separation (public/private VPC). Defense mechanisms: Security Groups vs NACLs, AWS WAF, Shield, Network Firewall. Workload security: EC2, ECS/EKS at basic level. Pillar 4 — Data Protection At-rest and in-transit data encryption (S3, EBS, RDS, DynamoDB). KMS for key management; Secrets Manager and Parameter Store for secrets management. Data classification and access guardrails setup. Pillar 5 — Incident Response Incident Response lifecycle according to AWS. Build IR playbook and automate with Lambda/Step Functions. Sample scenario: exposed IAM keys, S3 public exposure, malware detection on EC2. 3. Learnings Understand the 5 pillars of Security Pillar and Shared Responsibility Model. Apply advanced IAM: Identity Center, SCPs, avoid using long-term credentials. Understand the importance of Data Security: KMS, secrets management. Know how to build and automate Incident Response through serverless workflows. 4. Experience the event The workshop summarizes the learning chain, strengthening the security foundation before completing the project. The IAM Identity Center and Secrets Manager sections directly solve the problem of authentication and API key management of the group. IR scenarios (like S3 public exposure) help reinforce project security policies. Q\u0026amp;A session to further guide the AWS Security Specialty certification path. "},{"uri":"https://nguyentanxuan.github.io/fcj/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - OpenAI Open-Weight Models Now Available on AWS This blog introduces the availability of OpenAI’s open-weight models on AWS, enabling developers and organizations to build AI applications with greater flexibility, scalability, and transparency. You will learn how these models support text generation, reasoning, programming, and scientific workloads, how to access them through Amazon Bedrock and Amazon SageMaker JumpStart, and how features like 128K token context length and reasoning trace transparency can be leveraged to integrate AI into scalable, secure, and production-ready workflows.\nBlog 2 - AWS FOR SAP The AWS SAP Auto Scaling solution reduces EC2 costs by up to 50% by dynamically adjusting Application Servers based on real-time workload demand. It eliminates over-provisioning, scaling resources up or down while protecting active jobs via Soft Shutdown protocols. This automated approach optimizes efficiency and performance for enterprises with fluctuating usage patterns.\nBlog 3 - Measuring the Value of Learning \u0026amp; Development (L\u0026amp;D) Programs This blog demonstrate the ROI of Learning \u0026amp; Development (L\u0026amp;D), leaders must link upskilling directly to business outcomes rather than just tracking engagement metrics. This involves identifying specific organizational problems, aligning training with existing KPIs, and verifying that employees apply new skills to mission-critical tasks. By starting with clear metrics on skill acquisition and application, organizations can build trust with stakeholders and prove the tangible value of their learning programs.\n"},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand AWS Database services and their use cases. Learn about AWS Security concepts and services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Amazon RDS and Aurora - Learn about Multi-AZ Deployment, Read Replica, Automatic Backup, Snapshot 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice monitoring tools: CloudWatch, Enhanced Monitoring, Performance Insight - Introduction to AWS DMS \u0026amp; Schema Conversion Tool for database migration 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon DynamoDB (NoSQL) - Understand Purpose-built Database concept and suitable use cases 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Data Lake concepts on AWS - Services: S3, Glue Crawler, Glue Data Catalog, Athena 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Security basics: Shared Responsibility Model - Learn AWS IAM for identity \u0026amp; access management - Explore AWS Organizations and AWS SSO for account \u0026amp; identity management 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ 7 - Introduction to Compliance \u0026amp; Security Hub - Review overall AWS Security practices and governance 09/27/2025 09/27/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood Amazon RDS, Aurora, DynamoDB, and Purpose-built Databases. Practiced monitoring with CloudWatch, Enhanced Monitoring, and Performance Insight. Learned database migration tools: AWS DMS \u0026amp; Schema Conversion Tool. Gained knowledge about Data Lake architecture using S3, Glue, and Athena. Understood the Shared Responsibility Model for AWS Security. Used IAM for access management and explored AWS Organizations \u0026amp; SSO for multi-account governance. Introduced to Compliance \u0026amp; Security Hub for compliance and governance monitoring. "},{"uri":"https://nguyentanxuan.github.io/fcj/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 — AI/ML/GenAI on AWS\nTime: 09:00 November 15, 2025\nLocation: 26th Floor, Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 2 Event name: AWS Cloud Mastery Series #2 — DevOps on AWS\nTime: 09:00 November 17, 2025\nLocation: 26th Floor, Bitexco Building, No. 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 3 Event name: AWS Cloud Mastery Series #3 — According to AWS Well-Architected Security Pillar\nTime: 09:00 November 29/11/2025\nLocation: 26th Floor, Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\n"},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Connect and collaborate with team members to discuss the overall direction of the clothing e-commerce website project. Learn about the backend structure, technology stack, and define team roles. Start exploring how to integrate AWS services (such as S3, EC2, and RDS) into the backend system. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Meet team members and discuss project overview - Define main roles and responsibilities (Backend, Frontend, UI/UX, etc.) 09/29/2025 09/29/2025 Team meeting notes 3 - Explore backend project structure (Spring Boot) - Set up local development environment - Learn about existing API structure and database schema 09/30/2025 09/30/2025 Internal GitHub repo 4 - Research AWS services for potential integration + AWS S3 for image storage + AWS RDS for database hosting + AWS EC2 for backend deployment 10/01/2025 10/01/2025 AWS Documentation 5 - Discuss deployment strategy with the team - Plan backend endpoints related to product management (CRUD for clothing items) 10/02/2025 10/02/2025 Internal Docs 6 - Begin designing and coding API endpoints + Product listing + Category management + Image upload testing (with AWS S3) 10/03/2025 10/03/2025 Spring Boot Docs Week 4 Achievements: Connected and communicated effectively with team members to understand the project’s goals and workflow. Clearly defined the backend role and successfully set up the local development environment. Understood the overall architecture of the clothing e-commerce website, including database design, service layer, and API flow. Researched and evaluated AWS services for integration: AWS S3 — for storing product images. AWS RDS — for hosting the MySQL database. AWS EC2 — for backend deployment. Contributed to designing product-related APIs (CRUD operations). Successfully tested image upload and retrieval using AWS S3. Strengthened knowledge of backend workflows and AWS integration in real-world web applications. "},{"uri":"https://nguyentanxuan.github.io/fcj/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building Furious Five Fashion: AWS Full-Stack Infrastructure Workshop Overview The system architecture is built on a full-stack serverless model on AWS, focusing on automatic scalability, multi-layer security and cost optimization. All frontend – backend – data – AI – security components operate in a private environment, connected through VPC, PrivateLink and AWS management services\nYou will deploy seven CDK stacks linked together to create a scalable, secure and cost-optimized application:\nFrontend Layer – Deploy the interface on Amplify and distribute content via CloudFront.\nRouting \u0026amp; Protection – Protect access with Route 53, WAF and ACM SSL certificates.\nAuthentication Layer – Create a Cognito User Pool and integrate authentication for API Gateway.\nAPI Layer – Build a private API Gateway to securely communicate with the backend.\nCompute Layer – Run business logic using Lambda functions inside a private VPC.\nStorage Layer – Store static data and uploads on S3 via VPC Endpoint.\nData Layer – Run RDS in a private subnet and control access using IAM/SG.\nAI Layer – Integrate Amazon Bedrock to handle AI tasks via PrivateLink.\nSecurity \u0026amp; Observability – Monitor the entire system using CloudWatch, send alerts via SNS and manage security using IAM.\nContent Workshop Overview Setup Environment CDK Bootstrap Configure Infrastructure Stacks Configure API \u0026amp; Lambda Deploy Backend Services Test Endpoints End-to-End Push to GitLab Clean up "},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Begin shaping the system architecture for the e-commerce clothing website project. Research and design the system architecture integrated with AWS services. Visualize and document the project architecture using appropriate tools. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Review the project’s functional requirements.\n- Identify system modules and backend responsibilities. 06/10/2025 06/10/2025 Internal notes 2 - Research common web architectural models (3-tier, MVC, microservices, etc.). 07/10/2025 07/10/2025 AWS Documentation 3 - Learn AWS services used in the project:\n+ EC2\n+ RDS\n+ S3\n+ CloudFront\n+ CloudWatch 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Design and draw the system architecture diagram for the e-commerce project integrating AWS services. 09/10/2025 09/10/2025 draw.io 5 - Finalize and document the AWS-integrated architecture for group discussion and refinement. 10/10/2025 10/10/2025 Team meeting notes Week 5 Achievements: Understood the importance of system architecture in developing scalable web applications. Learned how to combine core AWS services (EC2, RDS, S3, CloudFront, CloudWatch) in an integrated architecture. Designed and drew the architecture diagram for the clothing e-commerce website with AWS integration. Defined how backend services interact with AWS for data storage, deployment, and monitoring. Documented the initial architecture for future implementation and team review. "},{"uri":"https://nguyentanxuan.github.io/fcj/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 8/9/2025 to 12/9/2025, I had the opportunity to apply my academic knowledge in a real-world professional setting while gaining practical experience.\nI participated in the First Cloud Journey program, which strengthened my communication abilities, enhanced my financial management skills, and expanded my understanding of AWS services and cloud computing.\nIn terms of professionalism, I consistently worked to complete all assigned tasks to a high standard, followed workplace guidelines, and actively collaborated with colleagues to improve team efficiency.\nNo. Criteria Description Good Fair Avg 1 Professional Knowledge \u0026amp; Skills Correctly understands the field, applies to practice, proficient with tools, high work quality ✅ ☐ ☐ 2 Learning Ability Absorbs new knowledge quickly, proactive in learning ☐ ✅ ☐ 3 Proactivity (Initiative) Self-starts tasks, does not wait for assignment ✅ ☐ ☐ 4 Sense of Responsibility Completes on time, ensures quality ✅ ☐ ☐ 5 Discipline Adheres to punctuality, regulations, and work processes ☐ ✅ ☐ 6 Progressive Spirit Willing to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presents ideas clearly, reports work coherently ☐ ✅ ☐ 8 Teamwork Cooperates effectively with colleagues, participates actively in team activities ✅ ☐ ☐ 9 Professionalism Respects colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving Skills Identifies problems, offers suitable solutions, possesses creative thinking ☐ ✅ ☐ 11 Contribution to Team/Project Work effectiveness, proposes new ideas, recognized by the team ☐ ✅ ☐ 12 Overall Assessment General evaluation of the entire internship process ☐ ✅ ☐ Needs Improvement Improve self-discipline and strictly comply with the company’s regulations. Enhance problem-solving thinking. Learn to handle situations more quickly and safely. Set clearer goals "},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Implement and test backend API endpoints for the clothing e-commerce website. Integrate AWS S3 for file storage and manage product images. Set up CI/CD pipeline for automated deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Implement product CRUD API endpoints - Test API using Postman or Insomnia - Validate request/response data 13/10/2025 13/10/2025 Spring Boot Documentation 2 - Set up AWS S3 bucket for product images - Configure bucket policies and CORS - Implement image upload/download functionality 14/10/2025 14/10/2025 AWS S3 Documentation 3 - Integrate file upload with product creation - Handle image resizing and optimization - Test image upload flow end-to-end 15/10/2025 15/10/2025 ImageIO, AWS SDK Documentation 4 - Set up GitHub Actions for CI/CD - Configure automated testing - Create deployment workflow 16/10/2025 16/10/2025 GitHub Actions Documentation 5 - Deploy application to AWS EC2 - Configure load balancer and auto-scaling - Set up monitoring with CloudWatch 17/10/2025 17/10/2025 AWS EC2, CloudWatch Documentation Week 6 Achievements: Successfully implemented complete CRUD operations for product management. Integrated AWS S3 for secure and scalable image storage. Configured proper bucket policies for secure file access. Built automated CI/CD pipeline using GitHub Actions. Deployed application to AWS EC2 with proper monitoring. Implemented image optimization to improve performance. Set up automated testing pipeline to ensure code quality. Configured CloudWatch for application monitoring and alerting. "},{"uri":"https://nguyentanxuan.github.io/fcj/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. Team members at FCJ are always willing to help whenever I face difficulties, even outside of working hours. The workspace is clean and comfortable, which helps me stay focused. However, I think the company could organize more social events or team-bonding activities to help everyone connect better.\n2. Support from Mentor / Team Admin\nMy mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports all necessary procedures, documents, and ensures I have what I need to work effectively. I really appreciate that my mentor allows me to try and solve problems on my own instead of giving the answer directly.\n3. Relevance of Work to Academic Major\nThe tasks I’m assigned are aligned with what I learned at university while also introducing me to new areas I haven\u0026rsquo;t explored before. This helps me reinforce my foundational knowledge while gaining more practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. My mentor also shared valuable real-world experiences that helped me shape my career path more clearly.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously, but maintains a cheerful atmosphere.. This made me feel like a true part of the team.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and allows flexible working hours when needed. Additionally, being able to join internal training sessions is a great advantage.\nAdditional Questions – Answers What were you most satisfied with during the internship?\nWhat I was most satisfied with was being given real tasks and receiving helpful feedback that allowed me to improve significantly.\nWhat do you think the company should improve for future interns?\nThe company could organize more onboarding workshops and technical training in the first week so new interns can adapt quickly.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes. Because the learning environment is great, mentors are supportive, and interns are given real opportunities to handle practical tasks instead of minor or unrelated work.\nSuggestions \u0026amp; Expectations – Answers Do you have any suggestions to improve the internship experience?\nThere should be more weekly check-in sessions between mentors and interns to track progress and provide clearer guidance.\nWould you like to continue this program in the future?\nYes, because this internship has provided me with valuable knowledge and motivation to grow further.\nAny additional comments (free sharing):\nI’m truly grateful for this opportunity. The internship has helped me grow in both technical skills and soft skills. Thank you to everyone on the team for your support.\n"},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Enhance e-commerce platform with user authentication and authorization. Implement shopping cart and order management functionality. Set up database optimization and security measures. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Implement JWT-based authentication system - Create user registration and login endpoints - Set up password encryption and validation 20/10/2025 20/10/2025 Spring Security Documentation 2 - Design and implement role-based access control - Create admin and customer user roles - Secure API endpoints with proper authorization 21/10/2025 21/10/2025 Spring Security, JWT Documentation 3 - Implement shopping cart functionality - Create cart session management - Handle add/remove/update cart items 22/10/2025 22/10/2025 Redis, Spring Session Documentation 4 - Build order management system - Create order placement and tracking - Implement order status workflow 23/10/2025 23/10/2025 Spring Boot, JPA Documentation 5 - Set up AWS RDS for production database - Configure database security groups - Implement database backup and monitoring 24/10/2025 24/10/2025 AWS RDS, CloudWatch Documentation Week 7 Achievements: Successfully implemented JWT-based authentication system with secure token management. Built comprehensive role-based access control for different user types. Created efficient shopping cart system with session management. Developed complete order management workflow from placement to fulfillment. Migrated database to AWS RDS with proper security configurations. Implemented automated backup strategies for data protection. Set up database monitoring and alerting through CloudWatch. Enhanced API security with proper authentication and authorization layers. "},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Implement payment integration and order processing system. Set up email notification service and real-time features. Optimize application performance and implement caching strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Integrate payment gateway (Stripe/PayPal) - Implement secure payment processing - Handle payment confirmation and failure scenarios 27/10/2025 27/10/2025 Stripe API, PayPal SDK Documentation 2 - Set up AWS SES for email notifications - Create email templates for orders - Implement order confirmation emails 28/10/2025 28/10/2025 AWS SES Documentation 3 - Implement real-time notifications using WebSocket - Set up order status updates - Create admin notification system 29/10/2025 29/10/2025 WebSocket, Spring WebSocket Documentation 4 - Set up Redis for caching - Implement product catalog caching - Add session caching for better performance 30/10/2025 30/10/2025 Redis, Spring Cache Documentation 5 - Optimize database queries - Implement pagination and filtering - Add search functionality with Elasticsearch 31/10/2025 31/10/2025 JPA, Elasticsearch Documentation Week 8 Achievements: Successfully integrated secure payment processing with multiple payment methods. Implemented comprehensive email notification system using AWS SES. Built real-time notification system for better user experience. Set up Redis caching to improve application performance significantly. Optimized database operations and implemented efficient search functionality. Created robust error handling for payment and order processing. Enhanced system scalability with proper caching strategies. Implemented monitoring for payment transactions and system performance. "},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.9-week9/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Continue learning and familiarizing yourself with AWS services Learn and implement projects Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Containerization with Docker - Container Orchestration with Amazon ECS - Container Orchestration with Amazon ECS 27/10/2025 27/10/2025 3 - CI/CD Pipeline with AWS CodePipeline - Automated Deployments with AWS CodePipeline - DevOps with AWS CodePipeline 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hybrid Storage with AWS Storage Gateway - Windows File Storage with Amazon FSx 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Windows File Storage with Amazon FSx - Building Advanced Applications with Amazon DynamoDB 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Workflow Orchestration with AWS Step Functions - Storage Performance Workshop 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Containerization with Docker\nBuild and push Docker images for backend + frontend, deploy on EC2.\nCreate Docker network and launch containers, test applications via public IP.\nUse Docker Compose to manage stacks and redeploy the entire system quickly.\nContainer Orchestration with Amazon ECS\nUnderstand Amazon ECS architecture, create and manage ECS clusters.\nDefine ECS tasks and services, configure Application Load Balancer for load balancing.\nDeploy containerized applications on ECS, test deployment and clean up resources after completion.\nContainer Orchestration with Amazon ECS\nBuild containerized Spring Boot (Java 21) microservices and deploy to ECS Fargate.\nUse AWS CDK to create VPC + NAT Gateway, ECS Cluster, Load Balancer, and API Gateway.\nCreate DynamoDB using CDK to serve as a backend storage for service “products”.\nInstrument ECS service with AWS X‑Ray (add sidecar, declare Inspector, test trace).\nClean up resources after completing the workshop.\nCI/CD Pipeline with AWS CodePipeline\nSet up automated CI/CD for containers on Amazon ECS, using GitLab, GitHub or CodeBuild.\nDeploy integration workflow (build → deploy) to ECS Service, ensuring automatic application updates.\nMonitor containers with Container Insights + collect logs via FireLens, ensuring effective observation and debugging.\nClean up AWS resources after completion to optimize costs.\nAutomated Deployments with AWS CodePipeline\nSet up a CodePipeline pipeline integrating CodeCommit → CodeBuild → CodeDeploy to deploy Node.js applications to EC2.\nConfigure CodeDeploy Agent on EC2 (via Session Manager or User Data).\nDeploy the application in-place to EC2 instances, test after deployment.\nSet up infrastructure (VPC, Security Group, RDS if needed) to serve the pipeline.\nClean up AWS resources after completing the lab to save costs.\nDevOps with AWS CodePipeline\nCreate an IAM Role for CodeBuild to interact with the EKS cluster.\nConfigure aws-auth to allow CodeBuild to use kubectl with permissions on the cluster.\nSet up a CodePipeline pipeline using CloudFormation to automatically build + deploy sample services to EKS.\nTrigger new releases when changing code on GitHub → automatically update services on EKS.\nClean up all resources (deployment, stack, bucket) after the lab to avoid additional costs.\nHybrid Storage with AWS Storage Gateway\nCreate S3 bucket + initialize EC2 to use as Storage Gateway.\nConfigure AWS Storage Gateway, create SMB/NFS file share and mount on on-premises machine.\nSet up SMB access (guest) to share files from on-prem machine to S3.\nTest by creating files on mounted drive → files are synchronized to S3.\nClean up resources: delete Gateway, EC2, S3 bucket after completion.\nWindows File Storage with Amazon FSx\nDeploy FSx Windows File Server using CloudFormation in VPC.\nCreate and map SMB file shares on EC2 Windows, test with sample data.\nEnable shadow copies to save previous file versions \u0026amp; configure backup storage.\nEnable Continuous Access share to support HA for applications such as SQL Server.\nAdjust storage capacity and throughput according to needs; clean up resources after doing lab\nBuilding Advanced Applications with Amazon DynamoDB\nDo hands-on DynamoDB lab to understand the basics of querying, Streams, Global Tables and NoSQL data model.\nBuild advanced “design patterns” for DynamoDB (partitioning, GSI, adjacency list).\nDeploy global serverless applications with DynamoDB Global Tables.\nWorkflow Orchestration with AWS Step Functions\nDesign and implement state machine orchestration using Lambda: including task, branching (Choice) and parallel (Parallel).\nConfigure retry \u0026amp; catch errors when Lambda encounters problems for more sustainable workflow.\nUse token callback (waitForTaskToken) to allow pause/resume workflow.\nSet up debugging / logging, visualize workflow and clean up resources after lab completion Storage Performance Workshop\nInitialize resources via CloudFormation (EC2, Security Group) to run performance lab.\nOptimize S3 throughput, use parallel prefix, sync command, and upload multiple small files to increase TPS.\nOptimize EFS performance: configure IOPS, I/O size, multi-threading and choose appropriate performance mode.\nClean up resources after lab: delete S3 bucket, terminate EC2, delete CloudFormation stack.\n"},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn more about aws services Continue working on the project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Serverless Backend with Lambda, S3, and DynamoDB - Frontend Development for Serverless APIs 03/11/2025 03/11/2025 3 - Deployment Automation with AWS SAM - User Authentication with Amazon Cognito 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Custom Domains and SSL for Serverless Applications - Event Processing with SQS and SNS 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - CI/CD for Serverless Applications - Monitoring Serverless Applications - Building GraphQL APIs with AWS AppSync 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Building Serverless APIs - Serverless Chat Application 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10Achievements: Frontend Development for Serverless APIs Build a web interface (frontend) to call API created by API Gateway + Lambda.\nDeploy front-end: can host static site (HTML/CSS/JS) connecting to serverless backend.\nDeploy Lambda function to handle request logic from API Gateway.\nConfigure API Gateway as endpoint for frontend: define route, HTTP method,…\nTest API with Postman: ensure API Gateway + Lambda runs correctly before frontend calls.\nTest frontend: web app calls API Gateway, receives results from Lambda and displays.\nDeployment Automation with AWS SAM Use AWS SAM (Serverless Application Model) to define serverless application via YAML file, then SAM will convert to CloudFormation when deploying.\nCreate and deploy front-end, Lambda function and API Gateway with SAM.\nTest API with Postman and frontend after deploying.\nUser Authentication with Amazon Cognito Create a Cognito User Pool to manage registration, login, email verification, password change, password reset.\nUse Identity Pool if you need to grant AWS Credentials (temporarily) to users to access services such as S3, DynamoDB.\nCreate API and Lambda: after the user authenticates via Cognito, Lambda handles the API request.\nFrontend: build login/registration UI, call Cognito API to log in and get token; then send token to API Gateway/Lambda.\nCustom Domains \u0026amp; SSL for Serverless Apps Use API Gateway to configure a custom domain name (“custom domain”) instead of the default hostname of API Gateway.\nUse Amazon Certificate Manager (ACM) to create or import SSL/TLS certificate for the domain, ensuring HTTPS.\nSelect custom domain type: edge-optimized (using CloudFront) or regional.\nConfigure Base Path Mapping to map domain + path to API Gateway stages.\nCreate DNS record (Route 53 or other DNS) to point domain to CloudFront distribution or endpoint provided by API Gateway.\nSSL and security: you can choose TLS security policy, ACM will manage certificate renewal. AWS Docs\n**Event Processing with SQS \u0026amp; SNS ** When users place an order, API sends messages to SQS queue to ensure “queue” to process the order.\nAt the same time, SNS is used to notify admin every time there is a new order.\nCreate Lambdas:\ncheckout_order to receive orders from API and push messages to SQS + publish SNS.\nhandle_order for admin to process orders: read from SQS, write orders to DynamoDB.\ndelete_order for admin to delete order: delete message in SQS.\norder_management for admin to view order list (using DynamoDB).\nArchitecture uses SQS for durability and asynchronous processing, SNS for fan-out notifications.\nCI/CD for Serverless Apps Use AWS SAM Pipelines to automate the process of building, packaging and deploying serverless applications.\nInitialize CI/CD pipeline: sam pipeline init to create pipeline configuration for AWS CodePipeline\nPipeline includes many stages: Source (Git), Build (using SAM CLI), Deploy (CloudFormation/SAM).\nUse build container image provided by AWS SAM to compile serverless app, helping reduce the effort of creating separate CI image.\nPipeline follows AWS best-practice pattern: supports multi-account, multi-region.\nMonitoring Serverless Applications Use CloudWatch Logs to debug Lambda: log invocations, errors, etc.\nCreate custom metrics in CloudWatch to monitor business or performance metrics.\nConfigure CloudWatch Alarms based on metrics to alert when there is an abnormal situation.\nUse AWS X-Ray to trace requests: draw service maps, see the path of requests and find bottlenecks.\n###Building GraphQL APIs with AWS AppSync\nUse AWS AppSync to create serverless GraphQL APIs, combined with AWS data sources such as DynamoDB, Lambda or Aurora Serverless.\nDefine GraphQL schema: type, query, mutation, subscription (if real-time required). AWS Docs\nConfigure resolvers:\nUse VTL mapping template to convert GraphQL requests into data source query operations.\nOr use Direct Lambda Resolver: Lambda handles GraphQL logic, avoiding the need to write VTL.\nIf using a relational database (Aurora Serverless), AppSync can connect via Data API to execute SQL commands via GraphQL mutation/query.\nAPI access can be managed in the following ways: AWS IAM, Cognito User Pools,… (authentication + authorization).\nBuilding Serverless APIs (Serverless with Lambda, API Gateway and SAM) Backend architecture uses Lambda, API Gateway, DynamoDB, S3, and Cognito.\nFrontend is a JavaScript application (Vue.js) hosted on S3 / Amplify.\nUse AWS SAM to define serverless resources and deploy backend (Lambda + API Gateway + DynamoDB)\nLambda reads / writes data from DynamoDB: for example, Lambda scans DynamoDB table to return list of “parks” (rides, attractions).\nDeploy API Gateway for frontend to call API; Lambda handles request logic.\nIn realtime: Lambda subscribes to SNS topic, saves information to DynamoDB, and forwards payload to IoT Core so frontend can receive real-time data.\nServerless Chat Application Build a serverless real-time chat application using API Gateway WebSocket, Lambda, and DynamoDB.\nWhen a client connects (“$connect”), Lambda stores the connection ID in DynamoDB.\nWhen a client disconnects (“$disconnect”), Lambda deletes the connection ID from DynamoDB.\nWhen a client sends a message (“sendmessage” route), Lambda reads the connection IDs from DynamoDB and uses the API Gateway Management API to broadcast the message to all connected clients.\n"},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Execute production deployment and go-live activities. Implement production monitoring and support procedures. Conduct user acceptance testing and bug fixes. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Execute production deployment to AWS - Configure production environment variables - Run database migration and data seeding 17/11/2025 17/11/2025 Deployment Checklist, AWS Best Practices 2 - Conduct user acceptance testing with stakeholders - Document and prioritize identified issues - Create bug tracking and resolution workflow 18/11/2025 18/11/2025 UAT Test Plans, JIRA/Bug Tracking Tools 3 - Implement critical bug fixes and patches - Deploy hotfixes to production environment - Verify system stability and performance 19/11/2025 19/11/2025 Bug Fixing Procedures 4 - Set up production monitoring dashboards - Configure alerting for critical system metrics - Establish on-call support procedures 20/11/2025 20/11/2025 CloudWatch, PagerDuty Documentation 5 - Conduct post-deployment review - Document lessons learned - Create operational runbooks and troubleshooting guides 21/11/2025 21/11/2025 Operations Documentation Week 11 Achievements: Successfully deployed clothing e-commerce platform to production. Completed user acceptance testing with stakeholder approval. Resolved all critical and high-priority bugs identified during testing. Established comprehensive production monitoring and alerting system. Created detailed operational procedures and support documentation. Implemented automated backup and recovery procedures. Set up 24/7 monitoring with appropriate escalation procedures. Achieved production stability with minimal downtime during deployment. "},{"uri":"https://nguyentanxuan.github.io/fcj/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete final project documentation and knowledge transfer. Conduct project retrospective and performance evaluation. Plan future enhancements and maintenance activities. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Complete comprehensive project documentation - Create technical architecture documentation - Document API specifications and user guides 24/11/2025 24/11/2025 Technical Writing Best Practices 2 - Conduct knowledge transfer sessions with team - Create video tutorials for system operations - Document troubleshooting procedures 25/11/2025 25/11/2025 Knowledge Transfer Templates 3 - Perform final system performance analysis - Generate usage reports and metrics - Create recommendations for future improvements 26/11/2025 26/11/2025 Performance Analysis Tools 4 - Conduct project retrospective meeting - Document lessons learned and best practices - Create improvement recommendations 27/11/2025 27/11/2025 Retrospective Meeting Templates 5 - Finalize all project deliverables - Submit final project report - Prepare presentation for stakeholders 28/11/2025 28/11/2025 Project Management Documentation Week 12 Achievements: Completed comprehensive project documentation covering all aspects of the system. Successfully conducted knowledge transfer to ensure smooth handover. Generated detailed performance analysis and optimization recommendations. Documented all lessons learned and best practices for future reference. Delivered complete clothing e-commerce platform with AWS integration. Achieved all project objectives within timeline and budget constraints. Established maintenance procedures and future enhancement roadmap. Received positive feedback from stakeholders on project delivery and quality. "},{"uri":"https://nguyentanxuan.github.io/fcj/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://nguyentanxuan.github.io/fcj/tags/","title":"Tags","tags":[],"description":"","content":""}]